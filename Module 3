============================================================================
MODULE 3: GPU CONFIGURATION FOR DETERMINISTIC INFERENCE
PURPOSE: GPU optimization supporting the core performance system
CORE PERFORMANCE: scripts/onload-trading achieves 4.37μs mean latency
LATENCY TARGET: <2ms variance in inference timing (complementary to core system)
STABILITY REQUIREMENT: Zero VRAM-related failures during trading hours
GPU HARDWARE: Dual RTX 6000 Pro Max Q (96GB VRAM each, 192GB total)
============================================================================

IMPORTANT: This module optimizes GPU inference. The primary trading latency 
breakthrough (4.37μs) is achieved by scripts/onload-trading through OnLoad 
kernel bypass + CPU isolation + zero-latency networking.

# -----------------------------------------------------------------------------
# OPERATION: Baseline Performance Measurement (Pre-Optimization)
# WHY: Establishes performance metrics before optimization to quantify actual improvement
# EXPECTED OUTCOME: Baseline inference timing, thermal profile, and VRAM usage recorded for both GPUs
# FAILURE MODE: If PyTorch not available, install with pip3 install torch torchvision torchaudio
# RTX 6000 PRO MAX Q LIMITATION: Professional cards show superior consistency but require enterprise cooling
# -----------------------------------------------------------------------------
mkdir -p ~/gpu_optimization_logs && cat > ~/baseline_measurement.py << 'EOF'
#!/usr/bin/env python3
import torch
import time
import statistics
import json
import subprocess
import os
from datetime import datetime

def measure_baseline_performance():
    results = {
        'timestamp': datetime.now().isoformat(),
        'system_info': {},
        'inference_timing': {},
        'thermal_profile': {},
        'vram_usage': {},
        'dual_gpu_config': {}
    }
    
    # System info for both GPUs
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        results['system_info']['gpu_count'] = gpu_count
        results['system_info']['cuda_version'] = torch.version.cuda
        results['system_info']['driver_version'] = subprocess.getoutput('nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits')
        
        for i in range(gpu_count):
            results['system_info'][f'gpu_{i}_name'] = torch.cuda.get_device_name(i)
    
    # Dual-GPU inference timing baseline
    device_0 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    device_1 = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cpu')
    
    # Large models for 96GB VRAM testing
    model_0 = torch.nn.Sequential(
        torch.nn.Linear(4096, 8192),
        torch.nn.ReLU(),
        torch.nn.Linear(8192, 4096)
    ).to(device_0)
    
    model_1 = torch.nn.Sequential(
        torch.nn.Linear(4096, 8192),
        torch.nn.ReLU(),
        torch.nn.Linear(8192, 4096)
    ).to(device_1)
    
    input_tensor_0 = torch.randn(128, 4096).to(device_0)
    input_tensor_1 = torch.randn(128, 4096).to(device_1)
    
    # Warmup both GPUs
    for _ in range(100):
        _ = model_0(input_tensor_0)
        _ = model_1(input_tensor_1)
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # Timing measurements for both GPUs
    timings_gpu0 = []
    timings_gpu1 = []
    
    for i in range(1000):
        # GPU 0 timing
        start = time.perf_counter()
        output_0 = model_0(input_tensor_0)
        if torch.cuda.is_available():
            torch.cuda.synchronize(device_0)
        end = time.perf_counter()
        timings_gpu0.append((end - start) * 1000)
        
        # GPU 1 timing
        start = time.perf_counter()
        output_1 = model_1(input_tensor_1)
        if torch.cuda.is_available():
            torch.cuda.synchronize(device_1)
        end = time.perf_counter()
        timings_gpu1.append((end - start) * 1000)
    
    results['inference_timing'] = {
        'gpu0': {
            'mean_ms': statistics.mean(timings_gpu0),
            'std_dev_ms': statistics.stdev(timings_gpu0),
            'variance_percent': statistics.stdev(timings_gpu0) / statistics.mean(timings_gpu0) * 100,
            'min_ms': min(timings_gpu0),
            'max_ms': max(timings_gpu0),
            'p95_ms': sorted(timings_gpu0)[int(0.95 * len(timings_gpu0))]
        },
        'gpu1': {
            'mean_ms': statistics.mean(timings_gpu1),
            'std_dev_ms': statistics.stdev(timings_gpu1),
            'variance_percent': statistics.stdev(timings_gpu1) / statistics.mean(timings_gpu1) * 100,
            'min_ms': min(timings_gpu1),
            'max_ms': max(timings_gpu1),
            'p95_ms': sorted(timings_gpu1)[int(0.95 * len(timings_gpu1))]
        }
    }
    
    # Thermal baseline for both GPUs
    if torch.cuda.is_available():
        temp_power_output = subprocess.getoutput('nvidia-smi --query-gpu=temperature.gpu,power.draw --format=csv,noheader,nounits')
        lines = temp_power_output.strip().split('\n')
        
        for i, line in enumerate(lines):
            if ', ' in line:
                temp, power = line.split(', ')
                results['thermal_profile'][f'gpu{i}'] = {
                    'idle_temp_c': int(temp),
                    'idle_power_w': int(power)
                }
        
        # VRAM usage baseline for both 96GB GPUs
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            memory_info = torch.cuda.mem_get_info()
            results['vram_usage'][f'gpu{i}'] = {
                'free_gb': memory_info[0] / 1e9,
                'total_gb': memory_info[1] / 1e9,
                'used_percent': (1 - memory_info[0] / memory_info[1]) * 100,
                'expected_total_gb': 96
            }
        
        # Total VRAM across both cards
        total_vram = sum([results['vram_usage'][f'gpu{i}']['total_gb'] for i in range(torch.cuda.device_count())])
        results['vram_usage']['total_system_gb'] = total_vram
    
    # Save baseline
    with open('~/gpu_optimization_logs/baseline_performance_dual_rtx6000.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("=== BASELINE PERFORMANCE MEASUREMENT (DUAL RTX 6000 PRO MAX Q) ===")
    print(f"GPU Count: {results['system_info'].get('gpu_count', 0)}")
    print(f"Total VRAM: {results['vram_usage'].get('total_system_gb', 0):.1f}GB (Expected: 192GB)")
    
    for i in range(results['system_info'].get('gpu_count', 0)):
        gpu_timing = results['inference_timing'].get(f'gpu{i}', {})
        gpu_thermal = results['thermal_profile'].get(f'gpu{i}', {})
        gpu_vram = results['vram_usage'].get(f'gpu{i}', {})
        
        print(f"\nGPU {i} ({results['system_info'].get(f'gpu_{i}_name', 'Unknown')}):")
        print(f"  Inference: {gpu_timing.get('mean_ms', 0):.3f}ms ± {gpu_timing.get('std_dev_ms', 0):.3f}ms")
        print(f"  Variance: {gpu_timing.get('variance_percent', 0):.2f}%")
        print(f"  Temperature: {gpu_thermal.get('idle_temp_c', 0)}°C")
        print(f"  VRAM: {gpu_vram.get('total_gb', 0):.1f}GB total, {gpu_vram.get('used_percent', 0):.1f}% used")
    
    print(f"\nBaseline saved to ~/gpu_optimization_logs/baseline_performance_dual_rtx6000.json")
    
    return results

if __name__ == "__main__":
    measure_baseline_performance()
EOF
chmod +x ~/baseline_measurement.py && python3 ~/baseline_measurement.py
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: Complete NVIDIA Environment Cleanup
# WHY: Remove conflicting drivers that cause non-deterministic GPU behavior in trading models
# EXPECTED OUTCOME: Clean system with no NVIDIA drivers or CUDA installations
# FAILURE MODE: If purge fails, boot with nomodeset and retry; check for secure boot conflicts
# RTX 6000 PRO MAX Q LIMITATION: Professional cards require certified drivers; avoid beta/experimental versions
# -----------------------------------------------------------------------------
sudo apt update && sudo apt purge '*nvidia*' '*cuda*' '*cublas*' '*curand*' '*cufft*' '*cufile*' '*curand*' '*cusolver*' '*cusparse*' '*npp*' '*nvjpeg*' -y && sudo apt autoremove -y && sudo apt autoclean && sudo rm -rf /usr/local/cuda* /opt/cuda* && sudo ldconfig && sudo rm -f /etc/modprobe.d/*nvidia* /etc/modprobe.d/*nouveau*
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: System Preparation for Dual GPU Determinism
# WHY: Install prerequisites and configure kernel modules for stable dual GPU operation
# EXPECTED OUTCOME: All build tools available, kernel headers match running kernel, nouveau blacklisted
# FAILURE MODE: If headers mismatch, reboot to latest kernel first
# RTX 6000 PRO MAX Q LIMITATION: Dual professional cards require enterprise-grade power delivery (minimum 1200W PSU)
# -----------------------------------------------------------------------------
sudo apt install build-essential linux-headers-$(uname -r) dkms gcc make pkg-config libvulkan1 mesa-vulkan-drivers vulkan-tools -y && echo 'blacklist nouveau' | sudo tee /etc/modprobe.d/blacklist-nouveau.conf && echo 'options nouveau modeset=0' | sudo tee -a /etc/modprobe.d/blacklist-nouveau.conf && sudo update-initramfs -u && sudo modprobe -r nouveau || true
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: NVIDIA Driver Installation (Version Pinned for Determinism)
# WHY: Version 535.154.05+ provides stable inference timing; newer versions may introduce variance
# EXPECTED OUTCOME: nvidia-smi shows version 535+ with both GPUs detected and persistence mode available
# FAILURE MODE: If installation fails, check secure boot (disable if necessary), verify both GPUs seated properly
# RTX 6000 PRO MAX Q LIMITATION: Professional cards require certified production drivers; avoid studio/game-ready variants
# -----------------------------------------------------------------------------
sudo ubuntu-drivers devices && sudo ubuntu-drivers install nvidia:535-open && sudo systemctl enable nvidia-persistenced && sudo reboot
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: CUDA Toolkit 12.3 Installation (Version Pinned)
# WHY: CUDA 12.3 provides optimal inference performance for trading models; 12.4+ may introduce latency
# EXPECTED OUTCOME: nvcc shows version 12.3, all CUDA libraries available in /usr/local/cuda-12.3
# FAILURE MODE: If installation conflicts, remove all CUDA first and retry
# RTX 6000 PRO MAX Q LIMITATION: Professional cards support full CUDA feature set; verify compute capability 8.9
# -----------------------------------------------------------------------------
wget https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda_12.3.2_545.23.08_linux.run && sudo chmod +x cuda_12.3.2_545.23.08_linux.run && sudo ./cuda_12.3.2_545.23.08_linux.run --silent --toolkit --samples --no-opengl-libs && rm cuda_12.3.2_545.23.08_linux.run
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: CUDA Environment Configuration for Dual GPU Trading Models
# WHY: Proper paths ensure consistent CUDA library loading; improper paths cause model loading failures
# EXPECTED OUTCOME: CUDA commands available in PATH, libraries in LD_LIBRARY_PATH
# FAILURE MODE: If nvcc not found after reboot, manually source ~/.bashrc
# RTX 6000 PRO MAX Q LIMITATION: Professional cards support advanced memory management features requiring proper environment
# -----------------------------------------------------------------------------
echo 'export PATH=/usr/local/cuda-12.3/bin:$PATH' >> ~/.bashrc && echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc && echo 'export CUDA_HOME=/usr/local/cuda-12.3' >> ~/.bashrc && echo 'export CUDA_VISIBLE_DEVICES=0,1' >> ~/.bashrc && source ~/.bashrc && sudo ldconfig
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: Dual GPU Clock Locking for Deterministic Performance
# WHY: Prevents thermal throttling that causes 5-15ms inference variance during trading hours
# EXPECTED OUTCOME: Both GPU clocks locked at maximum stable frequency, no throttling under load
# FAILURE MODE: If clocks won't lock, reduce target frequency by 50MHz increments until stable
# RTX 6000 PRO MAX Q LIMITATION: Professional cooling design allows higher sustained clocks than consumer cards
# -----------------------------------------------------------------------------
sudo nvidia-smi -pm 1 && sudo nvidia-smi -i 0 -ac $(nvidia-smi -i 0 --query-supported-clocks=memory,graphics --format=csv,noheader,nounits | head -1 | tr ',' ' ') && sudo nvidia-smi -i 1 -ac $(nvidia-smi -i 1 --query-supported-clocks=memory,graphics --format=csv,noheader,nounits | head -1 | tr ',' ' ') && sudo nvidia-smi -i 0 -pl $(nvidia-smi -i 0 --query-gpu=power.max_limit --format=csv,noheader,nounits | head -1) && sudo nvidia-smi -i 1 -pl $(nvidia-smi -i 1 --query-gpu=power.max_limit --format=csv,noheader,nounits | head -1) && sudo nvidia-settings -a '[gpu:0]/GPUPowerMizerMode=1' && sudo nvidia-settings -a '[gpu:1]/GPUPowerMizerMode=1' 2>/dev/null || true
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: PyTorch Deterministic Configuration for Dual GPU Trading Models
# WHY: Ensures consistent inference timing across model runs for trading decision reliability
# EXPECTED OUTCOME: PyTorch configured for deterministic operation with minimal performance impact
# FAILURE MODE: If performance drops >10%, disable cudnn.benchmark but keep deterministic=True
# RTX 6000 PRO MAX Q LIMITATION: Professional cards maintain deterministic performance better than consumer variants
# -----------------------------------------------------------------------------
cat > ~/pytorch_deterministic_config.py << 'EOF'
#!/usr/bin/env python3
import torch
import numpy as np
import random
import os

def configure_deterministic_pytorch():
    """Configure PyTorch for deterministic inference in dual RTX 6000 Pro Max Q trading applications"""
    
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    np.random.seed(42)
    random.seed(42)
    
    # Enable deterministic operations
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False  # Disable for consistency
    torch.use_deterministic_algorithms(True, warn_only=True)
    
    # Set environment variables for deterministic behavior
    os.environ['PYTHONHASHSEED'] = '42'
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Ensure both GPUs visible
    
    # Verify dual GPU configuration
    gpu_count = torch.cuda.device_count()
    total_vram = 0
    
    print("PyTorch configured for deterministic inference on dual RTX 6000 Pro Max Q")
    print(f"CUDA available: {torch.cuda.is_available()}")
    print(f"GPU count: {gpu_count}")
    print(f"Deterministic mode: {torch.backends.cudnn.deterministic}")
    print(f"cuDNN benchmark: {torch.backends.cudnn.benchmark}")
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        memory_total = torch.cuda.get_device_properties(i).total_memory / 1e9
        total_vram += memory_total
        print(f"GPU {i}: {gpu_name} ({memory_total:.1f}GB VRAM)")
    
    print(f"Total VRAM: {total_vram:.1f}GB (Expected: 192GB)")
    
    if total_vram < 190:
        print("WARNING: Total VRAM below expected 192GB - verify RTX 6000 Pro Max Q detection")
    
    return True

if __name__ == "__main__":
    configure_deterministic_pytorch()
EOF
chmod +x ~/pytorch_deterministic_config.py && python3 ~/pytorch_deterministic_config.py
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: Advanced Dual GPU VRAM Monitoring System with CPU Fallback
# WHY: Prevents VRAM exhaustion crashes; switches to CPU models automatically to maintain trading
# EXPECTED OUTCOME: Real-time VRAM monitoring for 192GB total with automatic model switching at 90% utilization
# FAILURE MODE: If monitoring fails, check nvidia-ml-py installation and GPU permissions
# RTX 6000 PRO MAX Q LIMITATION: 96GB per card enables massive model loading; monitoring prevents memory fragmentation
# -----------------------------------------------------------------------------
pip3 install nvidia-ml-py3 psutil torch && cat > ~/vram_monitor_dual_rtx6000.py << 'EOF'
#!/usr/bin/env python3
import nvidia_ml_py3 as nvml
import torch
import time
import logging
import subprocess
import json
import threading
from datetime import datetime, timedelta
from collections import deque

logging.basicConfig(level=logging.INFO, format='%(asctime)s - DUAL_VRAM - %(message)s')
nvml.nvmlInit()

class DualRTX6000VRAMMonitor:
    def __init__(self, threshold=0.90, critical=0.95):
        self.threshold = threshold  # 90% for 96GB cards
        self.critical = critical    # 95% for 96GB cards
        self.device_0 = nvml.nvmlDeviceGetHandleByIndex(0)
        self.device_1 = nvml.nvmlDeviceGetHandleByIndex(1)
        self.usage_history = deque(maxlen=3600)  # 1 hour of data
        self.fallback_active = False
        self.expected_vram_per_gpu = 96.0  # 96GB per RTX 6000 Pro Max Q
        
    def get_dual_vram_stats(self):
        """Get current VRAM usage statistics for both RTX 6000 Pro Max Q GPUs"""
        mem_info_0 = nvml.nvmlDeviceGetMemoryInfo(self.device_0)
        mem_info_1 = nvml.nvmlDeviceGetMemoryInfo(self.device_1)
        
        total_used = mem_info_0.used + mem_info_1.used
        total_free = mem_info_0.free + mem_info_1.free
        total_memory = mem_info_0.total + mem_info_1.total
        
        return {
            'gpu0': {
                'used_gb': mem_info_0.used / 1e9,
                'free_gb': mem_info_0.free / 1e9,
                'total_gb': mem_info_0.total / 1e9,
                'usage_percent': mem_info_0.used / mem_info_0.total
            },
            'gpu1': {
                'used_gb': mem_info_1.used / 1e9,
                'free_gb': mem_info_1.free / 1e9,
                'total_gb': mem_info_1.total / 1e9,
                'usage_percent': mem_info_1.used / mem_info_1.total
            },
            'total': {
                'used_gb': total_used / 1e9,
                'free_gb': total_free / 1e9,
                'total_gb': total_memory / 1e9,
                'usage_percent': total_used / total_memory,
                'expected_total_gb': 192.0
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def log_vram_usage(self):
        """Continuously log VRAM usage for both GPUs"""
        stats = self.get_dual_vram_stats()
        self.usage_history.append(stats)
        
        # Save hourly usage patterns
        if len(self.usage_history) >= 3600:
            filename = f'~/gpu_optimization_logs/dual_vram_usage_{datetime.now().strftime("%Y%m%d_%H")}.json'
            with open(filename, 'w') as f:
                json.dump(list(self.usage_history), f, indent=2)
    
    def trigger_cpu_fallback(self, total_usage_percent):
        """Switch models to CPU instead of killing processes"""
        if self.fallback_active:
            return
            
        self.fallback_active = True
        logging.critical(f"DUAL GPU VRAM CRITICAL ({total_usage_percent:.1%}) - Switching to CPU models")
        
        try:
            # Signal trading system to switch to CPU models
            with open('/tmp/trading_dual_vram_fallback', 'w') as f:
                json.dump({
                    'fallback_active': True,
                    'total_vram_usage': total_usage_percent,
                    'total_vram_gb': 192.0,
                    'timestamp': datetime.now().isoformat(),
                    'reason': 'DUAL_GPU_VRAM_CRITICAL'
                }, f)
            
            # Force garbage collection to free VRAM on both GPUs
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    torch.cuda.set_device(i)
                    torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            logging.info("CPU fallback activated - Trading system notified")
            
        except Exception as e:
            logging.error(f"Fallback activation failed: {e}")
    
    def check_recovery(self, total_usage_percent):
        """Check if we can recover from CPU fallback"""
        if not self.fallback_active:
            return
            
        if total_usage_percent < (self.threshold - 0.05):  # 5% buffer below threshold
            self.fallback_active = False
            logging.info(f"DUAL GPU VRAM recovered ({total_usage_percent:.1%}) - GPU models can resume")
            
            # Signal recovery
            with open('/tmp/trading_dual_vram_fallback', 'w') as f:
                json.dump({
                    'fallback_active': False,
                    'total_vram_usage': total_usage_percent,
                    'total_vram_gb': 192.0,
                    'timestamp': datetime.now().isoformat(),
                    'reason': 'DUAL_GPU_VRAM_RECOVERED'
                }, f)
    
    def validate_rtx6000_detection(self):
        """Validate that both RTX 6000 Pro Max Q GPUs are properly detected"""
        stats = self.get_dual_vram_stats()
        
        gpu0_vram = stats['gpu0']['total_gb']
        gpu1_vram = stats['gpu1']['total_gb']
        total_vram = stats['total']['total_gb']
        
        validation_results = {
            'gpu0_detected': gpu0_vram > 90,  # Should be ~96GB
            'gpu1_detected': gpu1_vram > 90,  # Should be ~96GB
            'total_vram_correct': total_vram > 180,  # Should be ~192GB
            'gpu0_vram_gb': gpu0_vram,
            'gpu1_vram_gb': gpu1_vram,
            'total_vram_gb': total_vram
        }
        
        if not all([validation_results['gpu0_detected'], validation_results['gpu1_detected'], validation_results['total_vram_correct']]):
            logging.error("RTX 6000 Pro Max Q validation FAILED:")
            logging.error(f"  GPU 0: {gpu0_vram:.1f}GB (Expected: ~96GB)")
            logging.error(f"  GPU 1: {gpu1_vram:.1f}GB (Expected: ~96GB)")
            logging.error(f"  Total: {total_vram:.1f}GB (Expected: ~192GB)")
            return False
        else:
            logging.info("RTX 6000 Pro Max Q validation PASSED:")
            logging.info(f"  GPU 0: {gpu0_vram:.1f}GB")
            logging.info(f"  GPU 1: {gpu1_vram:.1f}GB")
            logging.info(f"  Total: {total_vram:.1f}GB")
            return True
    
    def run_monitor(self):
        """Main monitoring loop for dual RTX 6000 Pro Max Q"""
        logging.info("Dual RTX 6000 Pro Max Q VRAM monitor started")
        
        # Validate GPU detection first
        if not self.validate_rtx6000_detection():
            logging.critical("GPU validation failed - stopping monitor")
            return
        
        while True:
            try:
                stats = self.get_dual_vram_stats()
                total_usage = stats['total']['usage_percent']
                gpu0_usage = stats['gpu0']['usage_percent']
                gpu1_usage = stats['gpu1']['usage_percent']
                
                # Log usage
                self.log_vram_usage()
                
                # Check thresholds
                if total_usage > self.critical:
                    self.trigger_cpu_fallback(total_usage)
                elif total_usage > self.threshold:
                    logging.warning(f"DUAL GPU VRAM HIGH: {total_usage:.1%} ({stats['total']['used_gb']:.1f}GB / 192GB)")
                    logging.warning(f"  GPU 0: {gpu0_usage:.1%} ({stats['gpu0']['used_gb']:.1f}GB)")
                    logging.warning(f"  GPU 1: {gpu1_usage:.1%} ({stats['gpu1']['used_gb']:.1f}GB)")
                else:
                    self.check_recovery(total_usage)
                
                time.sleep(1)
                
            except Exception as e:
                logging.error(f"Monitor error: {e}")
                time.sleep(5)

if __name__ == "__main__":
    monitor = DualRTX6000VRAMMonitor()
    monitor.run_monitor()
EOF
chmod +x ~/vram_monitor_dual_rtx6000.py
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: Dual GPU System Service Configuration with Monitoring
# WHY: Ensures GPU settings persist across reboots and VRAM monitoring runs automatically
# EXPECTED OUTCOME: Both GPU settings automatically applied at boot, dual VRAM monitoring active
# FAILURE MODE: If service fails, check systemctl status dual-gpu-trading-config for errors
# RTX 6000 PRO MAX Q LIMITATION: Professional cards require enterprise service management for optimal operation
# -----------------------------------------------------------------------------
sudo cat > /etc/systemd/system/dual-gpu-trading-config.service << 'EOF'
[Unit]
Description=Dual RTX 6000 Pro Max Q Configuration for Trading with VRAM Monitoring
After=graphical.target nvidia-persistenced.service
Wants=dual-vram-monitor.service

[Service]
Type=oneshot
ExecStart=/bin/bash -c 'nvidia-smi -pm 1 && nvidia-smi -i 0 -ac $(nvidia-smi -i 0 --query-supported-clocks=memory,graphics --format=csv,noheader,nounits | head -1 | tr "," " ") && nvidia-smi -i 1 -ac $(nvidia-smi -i 1 --query-supported-clocks=memory,graphics --format=csv,noheader,nounits | head -1 | tr "," " ") && nvidia-smi -i 0 -pl $(nvidia-smi -i 0 --query-gpu=power.max_limit --format=csv,noheader,nounits | head -1) && nvidia-smi -i 1 -pl $(nvidia-smi -i 1 --query-gpu=power.max_limit --format=csv,noheader,nounits | head -1)'
RemainAfterExit=yes
User=root

[Install]
WantedBy=multi-user.target
EOF

sudo cat > /etc/systemd/system/dual-vram-monitor.service << 'EOF'
[Unit]
Description=Dual RTX 6000 Pro Max Q VRAM Monitor for Trading
After=dual-gpu-trading-config.service
Requires=dual-gpu-trading-config.service

[Service]
Type=simple
ExecStart=/usr/bin/python3 /home/ChoubChoub/vram_monitor_dual_rtx6000.py
Restart=always
RestartSec=5
User=ChoubChoub
Environment=PYTHONPATH=/usr/local/lib/python3.11/site-packages

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload && sudo systemctl enable dual-gpu-trading-config.service dual-vram-monitor.service && sudo systemctl start dual-gpu-trading-config.service
```

```bash
# -----------------------------------------------------------------------------
# OPERATION: Post-Optimization Performance Measurement and Comparison
# WHY: Quantifies actual performance improvement to validate optimization effectiveness
# EXPECTED OUTCOME: Detailed before/after comparison showing <2ms variance achievement across both GPUs
# FAILURE MODE: If performance degraded, check thermal throttling and clock locking status
# RTX 6000 PRO MAX Q LIMITATION: Professional cards deliver superior consistency but require proper thermal management
# -----------------------------------------------------------------------------
cat > ~/performance_comparison_dual_rtx6000.py << 'EOF'
#!/usr/bin/env python3
import torch
import time
import statistics
import json
import subprocess
import os
from datetime import datetime

# Import deterministic configuration
exec(open('/home/ChoubChoub/pytorch_deterministic_config.py').read())

def measure_optimized_dual_performance():
    configure_deterministic_pytorch()
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'optimization_type': 'dual_rtx6000_pro_max_q_optimization',
        'system_info': {},
        'inference_timing': {},
        'thermal_profile': {},
        'vram_usage': {},
        'deterministic_config': {
            'cudnn_deterministic': torch.backends.cudnn.deterministic,
            'cudnn_benchmark': torch.backends.cudnn.benchmark,
            'use_deterministic_algorithms': True
        }
    }
    
    # System info for both RTX 6000 Pro Max Q GPUs
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        results['system_info']['gpu_count'] = gpu_count
        results['system_info']['cuda_version'] = torch.version.cuda
        results['system_info']['driver_version'] = subprocess.getoutput('nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits')
        
        # GPU clocks for both cards
        gpu0_clocks = subprocess.getoutput('nvidia-smi -i 0 --query-gpu=clocks.applications.graphics,clocks.applications.memory --format=csv,noheader,nounits')
        gpu1_clocks = subprocess.getoutput('nvidia-smi -i 1 --query-gpu=clocks.applications.graphics,clocks.applications.memory --format=csv,noheader,nounits')
        
        results['system_info']['gpu0_clocks'] = gpu0_clocks
        results['system_info']['gpu1_clocks'] = gpu1_clocks
        
        for i in range(gpu_count):
            results['system_info'][f'gpu_{i}_name'] = torch.cuda.get_device_name(i)
    
    # Large model inference testing for 96GB VRAM capacity
    device_0 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    device_1 = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cpu')
    
    # Large trading models suitable for 96GB VRAM
    model_0 = torch.nn.Sequential(
        torch.nn.Linear(8192, 16384),
        torch.nn.ReLU(),
        torch.nn.Linear(16384, 8192),
        torch.nn.ReLU(),
        torch.nn.Linear(8192, 4096)
    ).to(device_0)
    
    model_1 = torch.nn.Sequential(
        torch.nn.Linear(8192, 16384),
        torch.nn.ReLU(),
        torch.nn.Linear(16384, 8192),
        torch.nn.ReLU(),
        torch.nn.Linear(8192, 4096)
    ).to(device_1)
    
    input_tensor_0 = torch.randn(256, 8192).to(device_0)  # Larger batch for 96GB
    input_tensor_1 = torch.randn(256, 8192).to(device_1)
    
    # Extended warmup for stability
    for _ in range(200):
        _ = model_0(input_tensor_0)
        _ = model_1(input_tensor_1)
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # Measure with sustained load on both GPUs
    timings_gpu0 = []
    timings_gpu1 = []
    temps_gpu0 = []
    temps_gpu1 = []
    
    for i in range(2000):  # Extended test
        # GPU 0 timing
        start = time.perf_counter()
        output_0 = model_0(input_tensor_0)
        if torch.cuda.is_available():
            torch.cuda.synchronize(device_0)
        end = time.perf_counter()
        timings_gpu0.append((end - start) * 1000)
        
        # GPU 1 timing
        start = time.perf_counter()
        output_1 = model_1(input_tensor_1)
        if torch.cuda.is_available():
            torch.cuda.synchronize(device_1)
        end = time.perf_counter()
        timings_gpu1.append((end - start) * 1000)
        
        # Sample temperatures every 100 iterations
        if i % 100 == 0 and torch.cuda.is_available():
            temp_output = subprocess.getoutput('nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits')
            temp_lines = temp_output.strip().split('\n')
            if len(temp_lines) >= 2:
                temps_gpu0.append(int(temp_lines[0]))
                temps_gpu1.append(int(temp_lines[1]))
    
    # Calculate timing statistics for both GPUs
    for gpu_id, timings in [('gpu0', timings_gpu0), ('gpu1', timings_gpu1)]:
        results['inference_timing'][gpu_id] = {
            'mean_ms': statistics.mean(timings),
            'std_dev_ms': statistics.stdev(timings),
            'variance_percent': statistics.stdev(timings) / statistics.mean(timings) * 100,
            'min_ms': min(timings),
            'max_ms': max(timings),
            'p95_ms': sorted(timings)[int(0.95 * len(timings))],
            'p99_ms': sorted(timings)[int(0.99 * len(timings))],
            'target_achieved': statistics.stdev(timings) < 2.0
        }
    
    # Thermal profile under load for both GPUs
    for gpu_id, temps in [('gpu0', temps_gpu0), ('gpu1', temps_gpu1)]:
        if temps:
            results['thermal_profile'][gpu_id] = {
                'max_temp_c': max(temps),
                'mean_temp_c': statistics.mean(temps),
                'thermal_throttling_detected': max(temps) > 83  # RTX 6000 Pro Max Q threshold
            }
    
    # VRAM usage for both 96GB GPUs
    if torch.cuda.is_available():
        total_vram = 0
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            memory_info = torch.cuda.mem_get_info()
            gpu_vram = memory_info[1] / 1e9
            total_vram += gpu_vram
            
            results['vram_usage'][f'gpu{i}'] = {
                'free_gb': memory_info[0] / 1e9,
                'total_gb': gpu_vram,
                'used_percent': (1 - memory_info[0] / memory_info[1]) * 100,
                'expected_gb': 96.0
            }
        
        results['vram_usage']['total_system'] = {
            'total_gb': total_vram,
            'expected_gb': 192.0,
            'detection_accurate': total_vram > 190
        }
    
    # Save results
    with open('~/gpu_optimization_logs/optimized_performance_dual_rtx6000.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # Load baseline for comparison
    try:
        with open('~/gpu_optimization_logs/baseline_performance_dual_rtx6000.json', 'r') as f:
            baseline = json.load(f)
            
        print("=== DUAL RTX 6000 PRO MAX Q OPTIMIZATION RESULTS ===")
        print(f"GPU Count: {results['system_info'].get('gpu_count', 0)}")
        print(f"Total VRAM: {results['vram_usage']['total_system']['total_gb']:.1f}GB (Expected: 192GB)")
        print(f"Driver: {results['system_info'].get('driver_version', 'N/A')}")
        print()
        
        # Performance comparison for both GPUs
        for gpu_id in ['gpu0', 'gpu1']:
            if gpu_id in baseline['inference_timing'] and gpu_id in results['inference_timing']:
                baseline_gpu = baseline['inference_timing'][gpu_id]
                optimized_gpu = results['inference_timing'][gpu_id]
                
                print(f"{gpu_id.upper()} INFERENCE TIMING:")
                print(f"  Baseline:  {baseline_gpu['mean_ms']:.3f}ms ± {baseline_gpu['std_dev_ms']:.3f}ms ({baseline_gpu['variance_percent']:.2f}% variance)")
                print(f"  Optimized: {optimized_gpu['mean_ms']:.3f}ms ± {optimized_gpu['std_dev_ms']:.3f}ms ({optimized_gpu['variance_percent']:.2f}% variance)")
                print(f"  Improvement: {((baseline_gpu['std_dev_ms'] - optimized_gpu['std_dev_ms']) / baseline_gpu['std_dev_ms'] * 100):+.1f}% variance reduction")
                print(f"  Target (<2ms std dev): {'✓ ACHIEVED' if optimized_gpu['std_dev_ms'] < 2.0 else '✗ NOT ACHIEVED'}")
                print()
        
        # Thermal performance for both GPUs
        print("THERMAL PERFORMANCE:")
        for gpu_id in ['gpu0', 'gpu1']:
            baseline_temp = baseline['thermal_profile'].get(gpu_id, {}).get('idle_temp_c', 0)
            max_temp = results['thermal_profile'].get(gpu_id, {}).get('max_temp_c', 0)
            throttling = results['thermal_profile'].get(gpu_id, {}).get('thermal_throttling_detected', False)
            
            print(f"  {gpu_id.upper()}: Baseline idle {baseline_temp}°C → Max load {max_temp}°C")
            print(f"    Thermal throttling: {'✗ DETECTED' if throttling else '✓ NONE'}")
        print()
        
        print("CONFIGURATION:")
        print(f"  Deterministic mode: {'✓ ENABLED' if results['deterministic_config']['cudnn_deterministic'] else '✗ DISABLED'}")
        print(f"  GPU 0 clocks: {results['system_info'].get('gpu0_clocks', 'N/A')}")
        print(f"  GPU 1 clocks: {results['system_info'].get('gpu1_clocks', 'N/A')}")
        print(f"  VRAM detection: {'✓ ACCURATE' if results['vram_usage']['total_system']['detection_accurate'] else '✗ INACCURATE'}")
        
    except FileNotFoundError:
        print("Baseline measurements not found. Run baseline_measurement.py first.")
    
    return results

if __name__ == "__main__":
    measure_optimized_dual_performance()
EOF
chmod +x ~/performance_comparison_dual_rtx6000.py && python3 ~/performance_comparison_dual_rtx6000.py
```

=============================================================================
MODULE VALIDATION
-----------------------------------------------------------------------------
1. Verify NVIDIA drivers and CUDA installation with dual GPU detection

```bash
nvidia-smi && nvcc --version && python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, GPU count: {torch.cuda.device_count()}'); [print(f'GPU {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_memory/1e9:.1f}GB)') for i in range(torch.cuda.device_count())]; print(f'PyTorch version: {torch.__version__}')"
```

2. Validate deterministic configuration is active

```bash
python3 -c "import torch; print(f'Deterministic mode: {torch.backends.cudnn.deterministic}, Benchmark disabled: {not torch.backends.cudnn.benchmark}, Dual GPU visible: {torch.cuda.device_count() == 2}')"
```

3. Test dual GPU VRAM fallback mechanism (simulates VRAM pressure and verifies fallback)

```bash
timeout 30s python3 ~/vram_monitor_dual_rtx6000.py & sleep 5 && python3 -c "
import torch
try:
    # Create VRAM pressure on both GPUs
    large_tensor_0 = torch.randn(20000, 20000, device='cuda:0')
    large_tensor_1 = torch.randn(20000, 20000, device='cuda:1')
    print('Dual GPU VRAM pressure created')
    import time; time.sleep(10)
    # Check for fallback file
    import os
    if os.path.exists('/tmp/trading_dual_vram_fallback'):
        import json
        with open('/tmp/trading_dual_vram_fallback', 'r') as f:
            fallback_status = json.load(f)
        print(f'Dual GPU fallback mechanism test: {\"PASS\" if fallback_status.get(\"fallback_active\", False) else \"FAIL\"}')
    else:
        print('Dual GPU fallback mechanism test: FAIL - No fallback signal detected')
except Exception as e:
    print(f'Dual GPU VRAM test error: {e}')
finally:
    torch.cuda.empty_cache()
" && pkill -f vram_monitor_dual_rtx6000
```

4. Verify dual GPU persistence mode and clock locking status

```bash
nvidia-smi -q -d CLOCK,PERSISTENCE_MODE | grep -E "(Persistence Mode|Applications Clocks)" && echo "Dual GPU configuration validated"
```

5. Thermal stability test under sustained load on both GPUs (30-second stress test)

```bash
timeout 30s python3 -c "
import torch
import subprocess
import time
device_0 = torch.device('cuda:0')
device_1 = torch.device('cuda:1')
model_0 = torch.nn.Linear(4096, 4096).to(device_0)
model_1 = torch.nn.Linear(4096, 4096).to(device_1)
input_tensor_0 = torch.randn(128, 4096).to(device_0)
input_tensor_1 = torch.randn(128, 4096).to(device_1)
start_temps = subprocess.getoutput('nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits').strip().split('\n')
for i in range(1000):
    _ = model_0(input_tensor_0)
    _ = model_1(input_tensor_1)
    torch.cuda.synchronize()
end_temps = subprocess.getoutput('nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits').strip().split('\n')
for i, (start, end) in enumerate(zip(start_temps, end_temps)):
    print(f'GPU {i} thermal test: Start {start}°C → End {end}°C')
    print(f'GPU {i} thermal stability: {\"PASS\" if int(end) < 80 else \"WARNING - Monitor cooling\"}')
"
```

6. Final dual GPU performance validation with before/after comparison

```bash
python3 ~/performance_comparison_dual_rtx6000.py
```

=============================================================================
ROLLBACK PROCEDURE
-----------------------------------------------------------------------------
Expected Recovery Time: <90 seconds
Step 1: Stop all dual GPU services and monitoring

```bash
sudo systemctl stop dual-vram-monitor.service dual-gpu-trading-config.service && sudo systemctl disable dual-vram-monitor.service dual-gpu-trading-config.service && sudo rm -f /etc/systemd/system/dual-gpu-trading-config.service /etc/systemd/system/dual-vram-monitor.service
```

Step 2: Remove CUDA installation and dual GPU environment variables

```bash
sudo rm -rf /usr/local/cuda* && sudo apt purge 'cuda*' -y && sed -i '/cuda/d' ~/.bashrc && sed -i '/CUDA/d' ~/.bashrc && sed -i '/CUDA_VISIBLE_DEVICES/d' ~/.bashrc
```

Step 3: Remove NVIDIA drivers (system will fall back to nouveau)

```bash
sudo apt purge '*nvidia*' -y && sudo apt autoremove -y
```

Step 4: Remove dual GPU monitoring and configuration files

```bash
rm -f ~/vram_monitor_dual_rtx6000.py ~/pytorch_deterministic_config.py ~/performance_comparison_dual_rtx6000.py ~/baseline_measurement.py && rm -rf ~/gpu_optimization_logs && rm -f /tmp/trading_dual_vram_fallback
```

Step 5: Clean modprobe configuration and reboot

```bash
sudo rm -f /etc/modprobe.d/blacklist-nouveau.conf && sudo update-initramfs -u && sudo systemctl daemon-reload && sudo reboot
```

Verification: Confirm rollback success (run after reboot)

```bash
lsmod | grep nouveau && echo "Rollback successful - using open source nouveau drivers" && python3 -c "import torch; print(f'CUDA available: {torch.cuda.is_available()} (should be False after rollback), GPU count: {torch.cuda.device_count() if torch.cuda.is_available() else 0}')"
```

============================================================================
