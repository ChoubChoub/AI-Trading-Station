============================================================================
MODULE 3: GPU CONFIGURATION FOR DETERMINISTIC INFERENCE
PURPOSE: GPU optimization supporting the core performance system
CORE PERFORMANCE: scripts/onload-trading achieves 4.37μs mean latency
LATENCY TARGET: <2ms variance in inference timing (complementary to core system)
STABILITY REQUIREMENT: Zero VRAM-related failures during trading hours
GPU HARDWARE: Dual RTX 6000 Pro Max Q (96GB VRAM each, 192GB total)
============================================================================

IMPORTANT: This module optimizes GPU inference. The primary trading latency 
breakthrough (4.37μs) is achieved by scripts/onload-trading through OnLoad 
kernel bypass + CPU isolation + zero-latency networking.

## STEP 1: MEASURE YOUR CURRENT GPU PERFORMANCE (BASELINE)

**What we're doing:** Testing how fast your GPUs work right now, before any optimization.
**Why this matters:** We need to know your starting point to measure improvement later.
**Time required:** 2-3 minutes
**What you'll see:** Numbers showing how fast each GPU processes data and how much memory they use.

```bash
# -----------------------------------------------------------------------------
# OPERATION: Baseline Performance Measurement (Pre-Optimization)
# WHY: Establishes performance metrics before optimization to quantify actual improvement
# EXPECTED OUTCOME: Baseline inference timing, thermal profile, and VRAM usage recorded for both GPUs
# FAILURE MODE: If PyTorch not available, install with pip3 install torch torchvision torchaudio
# RTX 6000 PRO MAX Q LIMITATION: Professional cards show superior consistency but require enterprise cooling
# -----------------------------------------------------------------------------
mkdir -p ~/gpu_optimization_logs && cat > ~/baseline_measurement.py << 'EOF'
#!/usr/bin/env python3
import torch
import time
import statistics
import json
import subprocess
import os
from datetime import datetime

def measure_baseline_performance():
    print("=== MEASURING BASELINE PERFORMANCE FOR DUAL RTX 6000 PRO MAX Q ===")
    print("This will test both of your 96GB GPUs and take about 2 minutes...")
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'system_info': {},
        'inference_timing': {},
        'thermal_profile': {},
        'vram_usage': {},
        'dual_gpu_config': {}
    }
    
    # Check if we can see your GPUs
    if not torch.cuda.is_available():
        print("❌ ERROR: No CUDA GPUs detected. Make sure drivers are installed.")
        return None
    
    gpu_count = torch.cuda.device_count()
    print(f"✓ Found {gpu_count} GPU(s)")
    
    if gpu_count < 2:
        print("⚠️  WARNING: Expected 2 GPUs but found {gpu_count}. Continuing with available GPUs...")
    
    # Gather basic information about your system
    results['system_info']['gpu_count'] = gpu_count
    results['system_info']['cuda_version'] = torch.version.cuda
    results['system_info']['driver_version'] = subprocess.getoutput('nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits')
    
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        results['system_info'][f'gpu_{i}_name'] = gpu_name
        print(f"  GPU {i}: {gpu_name}")
    
    # Test each GPU individually
    print("\n🔄 Testing GPU performance...")
    
    for gpu_id in range(min(2, gpu_count)):  # Test up to 2 GPUs
        print(f"  Testing GPU {gpu_id}...")
        device = torch.device(f'cuda:{gpu_id}')
        
        # Create a medium-sized model to test with
        model = torch.nn.Sequential(
            torch.nn.Linear(4096, 8192),
            torch.nn.ReLU(),
            torch.nn.Linear(8192, 4096)
        ).to(device)
        
        input_tensor = torch.randn(128, 4096).to(device)
        
        # Warm up the GPU (this makes measurements more accurate)
        print(f"    Warming up GPU {gpu_id}...")
        for _ in range(100):
            _ = model(input_tensor)
        torch.cuda.synchronize(device)
        
        # Measure how fast the GPU processes data
        print(f"    Measuring GPU {gpu_id} speed...")
        timings = []
        for i in range(1000):
            start = time.perf_counter()
            output = model(input_tensor)
            torch.cuda.synchronize(device)
            end = time.perf_counter()
            timings.append((end - start) * 1000)  # Convert to milliseconds
        
        # Calculate statistics
        results['inference_timing'][f'gpu{gpu_id}'] = {
            'mean_ms': statistics.mean(timings),
            'std_dev_ms': statistics.stdev(timings),
            'variance_percent': statistics.stdev(timings) / statistics.mean(timings) * 100,
            'min_ms': min(timings),
            'max_ms': max(timings),
            'p95_ms': sorted(timings)[int(0.95 * len(timings))]
        }
    
    # Check temperature and power usage
    print("\n🌡️  Checking GPU temperatures and power...")
    temp_power_output = subprocess.getoutput('nvidia-smi --query-gpu=temperature.gpu,power.draw --format=csv,noheader,nounits')
    lines = temp_power_output.strip().split('\n')
    
    for i, line in enumerate(lines[:2]):  # Only check first 2 GPUs
        if ', ' in line:
            temp, power = line.split(', ')
            results['thermal_profile'][f'gpu{i}'] = {
                'idle_temp_c': int(temp),
                'idle_power_w': int(power)
            }
            print(f"  GPU {i}: {temp}°C, {power}W")
    
    # Check VRAM (memory) usage
    print("\n💾 Checking VRAM usage...")
    total_vram = 0
    for i in range(min(2, gpu_count)):
        torch.cuda.set_device(i)
        memory_info = torch.cuda.mem_get_info()
        free_gb = memory_info[0] / 1e9
        total_gb = memory_info[1] / 1e9
        used_percent = (1 - memory_info[0] / memory_info[1]) * 100
        total_vram += total_gb
        
        results['vram_usage'][f'gpu{i}'] = {
            'free_gb': free_gb,
            'total_gb': total_gb,
            'used_percent': used_percent,
            'expected_total_gb': 96
        }
        
        print(f"  GPU {i}: {total_gb:.1f}GB total ({used_percent:.1f}% used)")
        
        if total_gb < 90:
            print(f"    ⚠️  WARNING: Expected ~96GB but found {total_gb:.1f}GB")
    
    results['vram_usage']['total_system_gb'] = total_vram
    print(f"  Total VRAM: {total_vram:.1f}GB (Expected: 192GB)")
    
    # Save results to file
    with open(os.path.expanduser('~/gpu_optimization_logs/baseline_performance_dual_rtx6000.json'), 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\n✅ Baseline measurement complete!")
    print("📁 Results saved to ~/gpu_optimization_logs/baseline_performance_dual_rtx6000.json")
    
    # Show summary
    print("\n=== SUMMARY ===")
    for gpu_id in range(min(2, gpu_count)):
        if f'gpu{gpu_id}' in results['inference_timing']:
            timing = results['inference_timing'][f'gpu{gpu_id}']
            thermal = results['thermal_profile'].get(f'gpu{gpu_id}', {})
            vram = results['vram_usage'].get(f'gpu{gpu_id}', {})
            
            print(f"GPU {gpu_id}:")
            print(f"  Speed: {timing['mean_ms']:.3f}ms ± {timing['std_dev_ms']:.3f}ms")
            print(f"  Consistency: {timing['variance_percent']:.2f}% variance")
            print(f"  Temperature: {thermal.get('idle_temp_c', 'Unknown')}°C")
            print(f"  VRAM: {vram.get('total_gb', 0):.1f}GB")
    
    return results

if __name__ == "__main__":
    measure_baseline_performance()
EOF
chmod +x ~/baseline_measurement.py && python3 ~/baseline_measurement.py
```

## STEP 2: CLEAN UP OLD GPU DRIVERS

**What we're doing:** Removing any old or conflicting GPU drivers from your system.
**Why this matters:** Old drivers can cause inconsistent performance and crashes during trading.
**Time required:** 2-3 minutes
**What you'll see:** System removing old NVIDIA software and CUDA installations.

```bash
# -----------------------------------------------------------------------------
# OPERATION: Complete NVIDIA Environment Cleanup
# WHY: Remove conflicting drivers that cause non-deterministic GPU behavior in trading models
# EXPECTED OUTCOME: Clean system with no NVIDIA drivers or CUDA installations
# FAILURE MODE: If purge fails, boot with nomodeset and retry; check for secure boot conflicts
# RTX 6000 PRO MAX Q LIMITATION: Professional cards require certified drivers; avoid beta/experimental versions
# -----------------------------------------------------------------------------
echo "=== STEP 2: CLEANING UP OLD GPU DRIVERS ==="
echo "This will remove any existing NVIDIA drivers and CUDA installations..."
echo "⚠️  Your display may flicker briefly during this process."
echo ""
echo "Press Enter to continue or Ctrl+C to cancel..."
read

sudo apt update && sudo apt purge '*nvidia*' '*cuda*' '*cublas*' '*curand*' '*cufft*' '*cufile*' '*curand*' '*cusolver*' '*cusparse*' '*npp*' '*nvjpeg*' -y && sudo apt autoremove -y && sudo apt autoclean && sudo rm -rf /usr/local/cuda* /opt/cuda* && sudo ldconfig && sudo rm -f /etc/modprobe.d/*nvidia* /etc/modprobe.d/*nouveau*

echo "✅ Cleanup complete! Old drivers and CUDA removed."
```

## STEP 3: PREPARE YOUR SYSTEM FOR NEW GPU DRIVERS

**What we're doing:** Installing the tools needed to build and install new GPU drivers.
**Why this matters:** Your system needs these tools to properly install and configure the RTX 6000 Pro Max Q drivers.
**Time required:** 3-5 minutes
**What you'll see:** System downloading and installing development tools and kernel headers.

```bash
# -----------------------------------------------------------------------------
# OPERATION: System Preparation for Dual GPU Determinism
# WHY: Install prerequisites and configure kernel modules for stable dual GPU operation
# EXPECTED OUTCOME: All build tools available, kernel headers match running kernel, nouveau blacklisted
# FAILURE MODE: If headers mismatch, reboot to latest kernel first
# RTX 6000 PRO MAX Q LIMITATION: Dual professional cards require enterprise-grade power delivery (minimum 1200W PSU)
# -----------------------------------------------------------------------------
echo "=== STEP 3: PREPARING SYSTEM FOR GPU DRIVERS ==="
echo "Installing build tools and preparing your system..."
echo "This ensures your RTX 6000 Pro Max Q cards will work properly."
echo ""

sudo apt install build-essential linux-headers-$(uname -r) dkms gcc make pkg-config libvulkan1 mesa-vulkan-drivers vulkan-tools -y

echo ""
echo "🚫 Blacklisting nouveau driver (open-source driver that conflicts with NVIDIA)..."
echo 'blacklist nouveau' | sudo tee /etc/modprobe.d/blacklist-nouveau.conf
echo 'options nouveau modeset=0' | sudo tee -a /etc/modprobe.d/blacklist-nouveau.conf
sudo update-initramfs -u
sudo modprobe -r nouveau || true

echo "✅ System preparation complete!"
echo "📋 Installed: build tools, kernel headers, development packages"
echo "🚫 Blacklisted: nouveau driver (prevents conflicts)"
```

## STEP 4: INSTALL NVIDIA DRIVERS FOR YOUR RTX 6000 PRO MAX Q CARDS

**What we're doing:** Installing the official NVIDIA drivers that work best with your professional GPUs.
**Why this matters:** These specific drivers (version 535+) give the most consistent performance for trading applications.
**Time required:** 5-10 minutes + reboot time
**What you'll see:** Driver installation progress, then your system will restart automatically.

```bash
# -----------------------------------------------------------------------------
# OPERATION: NVIDIA Driver Installation (Version Pinned for Determinism)
# WHY: Version 535.154.05+ provides stable inference timing; newer versions may introduce variance
# EXPECTED OUTCOME: nvidia-smi shows version 535+ with both GPUs detected and persistence mode available
# FAILURE MODE: If installation fails, check secure boot (disable if necessary), verify both GPUs seated properly
# RTX 6000 PRO MAX Q LIMITATION: Professional cards require certified production drivers; avoid studio/game-ready variants
# -----------------------------------------------------------------------------
echo "=== STEP 4: INSTALLING NVIDIA DRIVERS ==="
echo "Installing official NVIDIA drivers for your RTX 6000 Pro Max Q cards..."
echo ""
echo "🔍 First, let's see what drivers are available for your system:"
sudo ubuntu-drivers devices

echo ""
echo "📥 Installing NVIDIA driver version 535 (recommended for trading stability)..."
echo "⚠️  Your system will automatically reboot after installation."
echo ""
echo "Press Enter to continue or Ctrl+C to cancel..."
read

sudo ubuntu-drivers install nvidia:535-open
sudo systemctl enable nvidia-persistenced

echo ""
echo "✅ Driver installation complete!"
echo "🔄 Your system will reboot in 10 seconds..."
echo "   After reboot, continue with Step 5."
echo ""
sleep 10
sudo reboot
```

## STEP 5: INSTALL CUDA TOOLKIT FOR AI PROCESSING

**What we're doing:** Installing CUDA 12.3, which is the software that lets your GPUs run AI trading models.
**Why this matters:** CUDA 12.3 is the most stable version for trading - newer versions might be slower.
**Time required:** 10-15 minutes
**What you'll see:** Download progress, then installation of CUDA libraries and tools.

```bash
# -----------------------------------------------------------------------------
# OPERATION: CUDA Toolkit 12.3 Installation (Version Pinned)
# WHY: CUDA 12.3 provides optimal inference performance for trading models; 12.4+ may introduce latency
# EXPECTED OUTCOME: nvcc shows version 12.3, all CUDA libraries available in /usr/local/cuda-12.3
# FAILURE MODE: If installation conflicts, remove all CUDA first and retry
# RTX 6000 PRO MAX Q LIMITATION: Professional cards support full CUDA feature set; verify compute capability 8.9
# -----------------------------------------------------------------------------
echo "=== STEP 5: INSTALLING CUDA TOOLKIT ==="
echo "Installing CUDA 12.3 for AI model processing on your RTX 6000 Pro Max Q cards..."
echo ""
echo "📋 What CUDA does:"
echo "   - Lets your GPUs run AI trading models"
echo "   - Provides libraries for neural network processing"
echo "   - Enables 96GB VRAM utilization per GPU"
echo ""

echo "📥 Downloading CUDA 12.3 installer (about 3.5GB)..."
echo "This may take several minutes depending on your internet speed..."
wget -c https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda_12.3.2_545.23.08_linux.run

echo ""
echo "🔧 Installing CUDA 12.3..."
echo "The installer will run silently (no questions asked)."
sudo chmod +x cuda_12.3.2_545.23.08_linux.run
sudo ./cuda_12.3.2_545.23.08_linux.run --silent --toolkit --samples --no-opengl-libs

echo ""
echo "🧹 Cleaning up installer file..."
rm cuda_12.3.2_545.23.08_linux.run

echo "✅ CUDA 12.3 installation complete!"
echo "📁 Installed to: /usr/local/cuda-12.3"
```

## STEP 6: CONFIGURE YOUR SYSTEM TO FIND CUDA

**What we're doing:** Setting up environment variables so your system knows where to find CUDA.
**Why this matters:** Without these settings, your trading software won't be able to use the GPUs.
**Time required:** 1 minute
**What you'll see:** Commands being added to your profile that run every time you log in.

```bash
# -----------------------------------------------------------------------------
# OPERATION: CUDA Environment Configuration for Dual GPU Trading Models
# WHY: Proper paths ensure consistent CUDA library loading; improper paths cause model loading failures
# EXPECTED OUTCOME: CUDA commands available in PATH, libraries in LD_LIBRARY_PATH
# FAILURE MODE: If nvcc not found after reboot, manually source ~/.bashrc
# RTX 6000 PRO MAX Q LIMITATION: Professional cards support advanced memory management features requiring proper environment
# -----------------------------------------------------------------------------
echo "=== STEP 6: CONFIGURING CUDA ENVIRONMENT ==="
echo "Setting up environment variables so your system can find CUDA..."
echo ""

echo "📝 Adding CUDA paths to your profile..."
echo 'export PATH=/usr/local/cuda-12.3/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
echo 'export CUDA_HOME=/usr/local/cuda-12.3' >> ~/.bashrc
echo 'export CUDA_VISIBLE_DEVICES=0,1' >> ~/.bashrc

echo "🔄 Loading new environment settings..."
source ~/.bashrc
sudo ldconfig

echo ""
echo "✅ CUDA environment configured!"
echo "📋 Added to your profile:"
echo "   - CUDA binaries in PATH"
echo "   - CUDA libraries in LD_LIBRARY_PATH"
echo "   - Both GPUs visible to applications"
echo ""
echo "🧪 Testing CUDA installation..."
if command -v nvcc &> /dev/null; then
    echo "✅ CUDA compiler found: $(nvcc --version | grep release)"
else
    echo "⚠️  CUDA compiler not found. You may need to restart your terminal."
fi
```

## STEP 7: LOCK GPU CLOCKS FOR CONSISTENT PERFORMANCE

**What we're doing:** Setting your GPUs to run at maximum speed all the time.
**Why this matters:** Prevents your GPUs from slowing down during trading, which would cause inconsistent timing.
**Time required:** 1-2 minutes
**What you'll see:** Commands that set and lock the clock speeds for both GPUs.

```bash
# -----------------------------------------------------------------------------
# OPERATION: Dual GPU Clock Locking for Deterministic Performance
# WHY: Prevents thermal throttling that causes 5-15ms inference variance during trading hours
# EXPECTED OUTCOME: Both GPU clocks locked at maximum stable frequency, no throttling under load
# FAILURE MODE: If clocks won't lock, reduce target frequency by 50MHz increments until stable
# RTX 6000 PRO MAX Q LIMITATION: Professional cooling design allows higher sustained clocks than consumer cards
# -----------------------------------------------------------------------------
echo "=== STEP 7: LOCKING GPU CLOCKS FOR CONSISTENT PERFORMANCE ==="
echo "Setting your RTX 6000 Pro Max Q cards to run at maximum stable speed..."
echo ""
echo "📋 Why we do this:"
echo "   - Prevents GPUs from changing speed during trading"
echo "   - Eliminates performance variations that could affect trading decisions"
echo "   - RTX 6000 Pro Max Q cards can handle sustained maximum performance"
echo ""

echo "⚡ Enabling persistence mode (keeps drivers loaded)..."
sudo nvidia-smi -pm 1

echo ""
echo "🔧 Setting maximum application clocks for both GPUs..."

# Get the maximum supported clocks for GPU 0
echo "  Getting maximum clocks for GPU 0..."
GPU0_CLOCKS=$(nvidia-smi -i 0 --query-supported-clocks=memory,graphics --format=csv,noheader,nounits | head -1 | tr ',' ' ')
echo "    GPU 0 max clocks: $GPU0_CLOCKS"
sudo nvidia-smi -i 0 -ac $GPU0_CLOCKS

# Get the maximum supported clocks for GPU 1
echo "  Getting maximum clocks for GPU 1..."
GPU1_CLOCKS=$(nvidia-smi -i 1 --query-supported-clocks=memory,graphics --format=csv,noheader,nounits | head -1 | tr ',' ' ')
echo "    GPU 1 max clocks: $GPU1_CLOCKS"
sudo nvidia-smi -i 1 -ac $GPU1_CLOCKS

echo ""
echo "🔋 Setting maximum power limits..."

# Set maximum power limit for GPU 0
GPU0_POWER=$(nvidia-smi -i 0 --query-gpu=power.max_limit --format=csv,noheader,nounits | head -1)
echo "  GPU 0 max power: ${GPU0_POWER}W"
sudo nvidia-smi -i 0 -pl $GPU0_POWER

# Set maximum power limit for GPU 1
GPU1_POWER=$(nvidia-smi -i 1 --query-gpu=power.max_limit --format=csv,noheader,nounits | head -1)
echo "  GPU 1 max power: ${GPU1_POWER}W"
sudo nvidia-smi -i 1 -pl $GPU1_POWER

echo ""
echo "🎯 Setting performance mode to maximum..."
sudo nvidia-settings -a '[gpu:0]/GPUPowerMizerMode=1' 2>/dev/null || true
sudo nvidia-settings -a '[gpu:1]/GPUPowerMizerMode=1' 2>/dev/null || true

echo ""
echo "✅ GPU clocks locked for consistent performance!"
echo "📊 Current GPU status:"
nvidia-smi --query-gpu=name,clocks.applications.graphics,clocks.applications.memory,power.limit --format=table
```

## STEP 8: CONFIGURE PYTORCH FOR CONSISTENT AI MODEL PERFORMANCE

**What we're doing:** Setting up PyTorch (the AI library) to run models the same way every time.
**Why this matters:** Ensures your trading models make consistent decisions - critical for financial applications.
**Time required:** 2 minutes
**What you'll see:** A script being created that configures PyTorch for deterministic behavior.

```bash
# -----------------------------------------------------------------------------
# OPERATION: PyTorch Deterministic Configuration for Dual GPU Trading Models
# WHY: Ensures consistent inference timing across model runs for trading decision reliability
# EXPECTED OUTCOME: PyTorch configured for deterministic operation with minimal performance impact
# FAILURE MODE: If performance drops >10%, disable cudnn.benchmark but keep deterministic=True
# RTX 6000 PRO MAX Q LIMITATION: Professional cards maintain deterministic performance better than consumer variants
# -----------------------------------------------------------------------------
echo "=== STEP 8: CONFIGURING PYTORCH FOR CONSISTENT AI PERFORMANCE ==="
echo "Setting up PyTorch to run AI models consistently on your dual RTX 6000 Pro Max Q..."
echo ""
echo "📋 What deterministic mode does:"
echo "   - AI models produce identical results every time"
echo "   - Critical for trading where consistency = reliability"
echo "   - Prevents random variations in trading decisions"
echo ""

cat > ~/pytorch_deterministic_config.py << 'EOF'
#!/usr/bin/env python3
import torch
import numpy as np
import random
import os

def configure_deterministic_pytorch():
    """Configure PyTorch for deterministic inference in dual RTX 6000 Pro Max Q trading applications"""
    
    print("🔧 Configuring PyTorch for deterministic AI inference...")
    
    # Set random seeds for reproducibility
    torch.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    np.random.seed(42)
    random.seed(42)
    
    # Enable deterministic operations
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False  # Disable for consistency
    torch.use_deterministic_algorithms(True, warn_only=True)
    
    # Set environment variables for deterministic behavior
    os.environ['PYTHONHASHSEED'] = '42'
    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
    os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Ensure both GPUs visible
    
    # Verify dual GPU configuration
    gpu_count = torch.cuda.device_count()
    total_vram = 0
    
    print("")
    print("=== PYTORCH CONFIGURATION COMPLETE ===")
    print(f"✅ CUDA available: {torch.cuda.is_available()}")
    print(f"✅ GPU count detected: {gpu_count}")
    print(f"✅ Deterministic mode: {torch.backends.cudnn.deterministic}")
    print(f"✅ Benchmark mode: {torch.backends.cudnn.benchmark} (should be False)")
    print("")
    
    if gpu_count >= 2:
        print("🎯 Dual GPU Configuration:")
        for i in range(min(2, gpu_count)):
            gpu_name = torch.cuda.get_device_name(i)
            memory_total = torch.cuda.get_device_properties(i).total_memory / 1e9
            total_vram += memory_total
            print(f"   GPU {i}: {gpu_name} ({memory_total:.1f}GB VRAM)")
        
        print(f"   Total VRAM: {total_vram:.1f}GB")
        
        if total_vram > 190:
            print("✅ RTX 6000 Pro Max Q detection: SUCCESS")
        else:
            print("⚠️  RTX 6000 Pro Max Q detection: WARNING - Expected ~192GB total")
    else:
        print("⚠️  Expected 2 GPUs but found {gpu_count}")
    
    print("")
    print("📋 Configuration Summary:")
    print("   - Random seeds: Fixed for reproducibility")
    print("   - Deterministic algorithms: Enabled")
    print("   - Performance optimization: Disabled (for consistency)")
    print("   - Both GPUs: Visible to applications")
    
    return True

if __name__ == "__main__":
    configure_deterministic_pytorch()
EOF

chmod +x ~/pytorch_deterministic_config.py

echo "🧪 Testing PyTorch configuration..."
python3 ~/pytorch_deterministic_config.py

echo ""
echo "✅ PyTorch configured for deterministic AI inference!"
echo "📁 Configuration script saved to: ~/pytorch_deterministic_config.py"
echo "💡 This script will be run automatically to ensure consistent AI model behavior."
```

## STEP 9: SET UP ADVANCED MEMORY MONITORING FOR 192GB VRAM

**What we're doing:** Creating a monitoring system that watches your 192GB of GPU memory and prevents crashes.
**Why this matters:** With 96GB per GPU, you need smart monitoring to prevent memory issues during heavy trading.
**Time required:** 3-5 minutes
**What you'll see:** A monitoring script being created that will run continuously to protect your system.

```bash
# -----------------------------------------------------------------------------
# OPERATION: Advanced Dual GPU VRAM Monitoring System with CPU Fallback
# WHY: Prevents VRAM exhaustion crashes; switches to CPU models automatically to maintain trading
# EXPECTED OUTCOME: Real-time VRAM monitoring for 192GB total with automatic model switching at 90% utilization
# FAILURE MODE: If monitoring fails, check nvidia-ml-py installation and GPU permissions
# RTX 6000 PRO MAX Q LIMITATION: 96GB per card enables massive model loading; monitoring prevents memory fragmentation
# -----------------------------------------------------------------------------
echo "=== STEP 9: SETTING UP ADVANCED MEMORY MONITORING ==="
echo "Creating a smart monitoring system for your 192GB of GPU memory..."
echo ""
echo "📋 What this monitoring system does:"
echo "   - Watches memory usage on both 96GB GPUs continuously"
echo "   - Automatically switches to CPU if GPU memory gets too full"
echo "   - Prevents crashes during heavy trading periods"
echo "   - Logs memory usage patterns for optimization"
echo ""

echo "📦 Installing required monitoring libraries..."
pip3 install nvidia-ml-py3 psutil torch

echo ""
echo "🛠️  Creating advanced VRAM monitoring script..."

cat > ~/vram_monitor_dual_rtx6000.py << 'EOF'
#!/usr/bin/env python3
import nvidia_ml_py3 as nvml
import torch
import time
import logging
import subprocess
import json
import threading
from datetime import datetime, timedelta
from collections import deque

# Set up logging for the monitor
logging.basicConfig(level=logging.INFO, format='%(asctime)s - DUAL_GPU_MONITOR - %(message)s')

try:
    nvml.nvmlInit()
    print("✅ NVIDIA monitoring library initialized successfully")
except Exception as e:
    print(f"❌ Failed to initialize NVIDIA monitoring: {e}")
    exit(1)

class DualRTX6000VRAMMonitor:
    def __init__(self, threshold=0.90, critical=0.95):
        """
        Initialize monitor for dual RTX 6000 Pro Max Q GPUs
        threshold: 90% - Start warning when memory usage exceeds this
        critical: 95% - Switch to CPU models when memory usage exceeds this
        """
        self.threshold = threshold
        self.critical = critical
        self.device_0 = nvml.nvmlDeviceGetHandleByIndex(0)
        self.device_1 = nvml.nvmlDeviceGetHandleByIndex(1)
        self.usage_history = deque(maxlen=3600)  # Store 1 hour of data
        self.fallback_active = False
        self.expected_vram_per_gpu = 96.0  # Expected 96GB per RTX 6000 Pro Max Q
        
        print(f"🎯 Monitor initialized with thresholds:")
        print(f"   Warning threshold: {threshold*100:.0f}% ({threshold*192:.1f}GB)")
        print(f"   Critical threshold: {critical*100:.0f}% ({critical*192:.1f}GB)")
    
    def get_dual_vram_stats(self):
        """Get current memory usage for both RTX 6000 Pro Max Q GPUs"""
        try:
            mem_info_0 = nvml.nvmlDeviceGetMemoryInfo(self.device_0)
            mem_info_1 = nvml.nvmlDeviceGetMemoryInfo(self.device_1)
            
            total_used = mem_info_0.used + mem_info_1.used
            total_free = mem_info_0.free + mem_info_1.free
            total_memory = mem_info_0.total + mem_info_1.total
            
            return {
                'gpu0': {
                    'used_gb': mem_info_0.used / 1e9,
                    'free_gb': mem_info_0.free / 1e9,
                    'total_gb': mem_info_0.total / 1e9,
                    'usage_percent': mem_info_0.used / mem_info_0.total
                },
                'gpu1': {
                    'used_gb': mem_info_1.used / 1e9,
                    'free_gb': mem_info_1.free / 1e9,
                    'total_gb': mem_info_1.total / 1e9,
                    'usage_percent': mem_info_1.used / mem_info_1.total
                },
                'total': {
                    'used_gb': total_used / 1e9,
                    'free_gb': total_free / 1e9,
                    'total_gb': total_memory / 1e9,
                    'usage_percent': total_used / total_memory,
                    'expected_total_gb': 192.0
                },
                'timestamp': datetime.now().isoformat(),
                'status': 'healthy'
            }
        except Exception as e:
            logging.error(f"Failed to get VRAM stats: {e}")
            return None
    
    def validate_rtx6000_detection(self):
        """Check that both RTX 6000 Pro Max Q GPUs are properly detected"""
        print("\n🔍 Validating RTX 6000 Pro Max Q detection...")
        
        stats = self.get_dual_vram_stats()
        if not stats:
            print("❌ Failed to get GPU memory statistics")
            return False
        
        gpu0_vram = stats['gpu0']['total_gb']
        gpu1_vram = stats['gpu1']['total_gb']
        total_vram = stats['total']['total_gb']
        
        print(f"   GPU 0: {gpu0_vram:.1f}GB VRAM")
        print(f"   GPU 1: {gpu1_vram:.1f}GB VRAM")
        print(f"   Total: {total_vram:.1f}GB VRAM")
        
        validation_passed = True
        
        if gpu0_vram < 90:
            print(f"⚠️  GPU 0: Expected ~96GB but found {gpu0_vram:.1f}GB")
            validation_passed = False
        
        if gpu1_vram < 90:
            print(f"⚠️  GPU 1: Expected ~96GB but found {gpu1_vram:.1f}GB")
            validation_passed = False
        
        if total_vram < 180:
            print(f"⚠️  Total: Expected ~192GB but found {total_vram:.1f}GB")
            validation_passed = False
        
        if validation_passed:
            print("✅ RTX 6000 Pro Max Q validation: PASSED")
        else:
            print("❌ RTX 6000 Pro Max Q validation: FAILED")
        
        return validation_passed
    
    def trigger_cpu_fallback(self, total_usage_percent):
        """Switch trading models to CPU when GPU memory is critical"""
        if self.fallback_active:
            return
        
        self.fallback_active = True
        logging.critical(f"🚨 CRITICAL: GPU memory at {total_usage_percent:.1%} - Switching to CPU models")
        
        try:
            # Create fallback signal file for trading system
            fallback_info = {
                'fallback_active': True,
                'total_vram_usage': total_usage_percent,
                'total_vram_gb': 192.0,
                'timestamp': datetime.now().isoformat(),
                'reason': 'DUAL_GPU_VRAM_CRITICAL',
                'action_required': 'Trading system should switch to CPU models'
            }
            
            with open('/tmp/trading_dual_vram_fallback', 'w') as f:
                json.dump(fallback_info, f, indent=2)
            
            # Try to free some GPU memory
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    torch.cuda.set_device(i)
                    torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            logging.info("✅ CPU fallback activated - Trading system notified")
            
        except Exception as e:
            logging.error(f"❌ Failed to activate fallback: {e}")
    
    def check_recovery(self, total_usage_percent):
        """Check if GPU memory has recovered enough to resume GPU models"""
        if not self.fallback_active:
            return
        
        recovery_threshold = self.threshold - 0.05  # 5% buffer below warning threshold
        
        if total_usage_percent < recovery_threshold:
            self.fallback_active = False
            logging.info(f"✅ GPU memory recovered ({total_usage_percent:.1%}) - GPU models can resume")
            
            # Signal recovery to trading system
            recovery_info = {
                'fallback_active': False,
                'total_vram_usage': total_usage_percent,
                'total_vram_gb': 192.0,
                'timestamp': datetime.now().isoformat(),
                'reason': 'DUAL_GPU_VRAM_RECOVERED',
                'action_available': 'Trading system can resume GPU models'
            }
            
            with open('/tmp/trading_dual_vram_fallback', 'w') as f:
                json.dump(recovery_info, f, indent=2)
    
    def run_monitor(self):
        """Main monitoring loop - runs continuously"""
        print("\n🚀 Starting dual RTX 6000 Pro Max Q VRAM monitor...")
        
        # Validate GPU detection first
        if not self.validate_rtx6000_detection():
            print("❌ GPU validation failed - stopping monitor")
            return
        
        print(f"\n📊 Monitoring started at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("   Press Ctrl+C to stop monitoring")
        print("")
        
        loop_count = 0
        
        try:
            while True:
                stats = self.get_dual_vram_stats()
                if not stats:
                    time.sleep(5)
                    continue
                
                total_usage = stats['total']['usage_percent']
                gpu0_usage = stats['gpu0']['usage_percent']
                gpu1_usage = stats['gpu1']['usage_percent']
                
                # Store usage history
                self.usage_history.append(stats)
                
                # Check thresholds and take action
                if total_usage > self.critical:
                    self.trigger_cpu_fallback(total_usage)
                elif total_usage > self.threshold:
                    logging.warning(f"⚠️  HIGH MEMORY: {total_usage:.1%} ({stats['total']['used_gb']:.1f}GB / 192GB)")
                    logging.warning(f"    GPU 0: {gpu0_usage:.1%} ({stats['gpu0']['used_gb']:.1f}GB)")
                    logging.warning(f"    GPU 1: {gpu1_usage:.1%} ({stats['gpu1']['used_gb']:.1f}GB)")
                else:
                    self.check_recovery(total_usage)
                
                # Print status every 60 loops (about 1 minute)
                if loop_count % 60 == 0:
                    print(f"📊 {datetime.now().strftime('%H:%M:%S')} - Total: {total_usage:.1%} | GPU0: {gpu0_usage:.1%} | GPU1: {gpu1_usage:.1%} | Status: {'FALLBACK' if self.fallback_active else 'NORMAL'}")
                
                loop_count += 1
                time.sleep(1)
                
        except KeyboardInterrupt:
            print("\n🛑 Monitor stopped by user")
        except Exception as e:
            logging.error(f"❌ Monitor error: {e}")
            print(f"Monitor encountered an error: {e}")

def test_monitor():
    """Test the monitoring system briefly"""
    print("🧪 Testing monitoring system for 10 seconds...")
    monitor = DualRTX6000VRAMMonitor()
    
    for i in range(10):
        stats = monitor.get_dual_vram_stats()
        if stats:
            total_usage = stats['total']['usage_percent']
            print(f"   Test {i+1}/10: {total_usage:.1%} memory usage")
        time.sleep(1)
    
    print("✅ Monitor test completed successfully!")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        test_monitor()
    else:
        monitor = DualRTX6000VRAMMonitor()
        monitor.run_monitor()
EOF

chmod +x ~/vram_monitor_dual_rtx6000.py

echo ""
echo "🧪 Testing the monitoring system..."
python3 ~/vram_monitor_dual_rtx6000.py test

echo ""
echo "✅ Advanced VRAM monitoring system created!"
echo "📁 Monitor script: ~/vram_monitor_dual_rtx6000.py"
echo "💡 The monitor will automatically:"
echo "   - Watch both 96GB GPUs continuously"
echo "   - Switch to CPU models if memory gets critical (>95%)"
echo "   - Log all memory usage for analysis"
echo "   - Recover automatically when memory usage drops"
```

## STEP 10: CREATE AUTOMATIC STARTUP SERVICES

**What we're doing:** Setting up your system to automatically configure GPUs and start monitoring when it boots.
**Why this matters:** Ensures your GPU optimization is always active, even after reboots or power outages.
**Time required:** 2-3 minutes
**What you'll see:** System services being created that will run automatically in the background.

```bash
# -----------------------------------------------------------------------------
# OPERATION: Dual GPU System Service Configuration with Monitoring
# WHY: Ensures GPU settings persist across reboots and VRAM monitoring runs automatically
# EXPECTED OUTCOME: Both GPU settings automatically applied at boot, dual VRAM monitoring active
# FAILURE MODE: If service fails, check systemctl status dual-gpu-trading-config for errors
# RTX 6000 PRO MAX Q LIMITATION: Professional cards require enterprise service management for optimal operation
# -----------------------------------------------------------------------------
echo "=== STEP 10: CREATING AUTOMATIC STARTUP SERVICES ==="
echo "Setting up services to automatically configure your GPUs and start monitoring..."
echo ""
echo "📋 Services being created:"
echo "   1. dual-gpu-trading-config.service - Configures GPU clocks and power on boot"
echo "   2. dual-vram-monitor.service - Starts memory monitoring automatically"
echo ""

echo "🔧 Creating GPU configuration service..."
sudo cat > /etc/systemd/system/dual-gpu-trading-config.service << 'EOF'
[Unit]
Description=Dual RTX 6000 Pro Max Q Configuration for Trading
After=graphical.target nvidia-persistenced.service
Wants=dual-vram-monitor.service

[Service]
Type=oneshot
ExecStart=/bin/bash -c 'echo "Configuring RTX 6000 Pro Max Q GPUs..." && nvidia-smi -pm 1 && nvidia-smi -i 0 -ac $(nvidia-smi -i 0 --query-supported-clocks=memory,graphics --format=csv,noheader,nounits | head -1 | tr "," " ") && nvidia-smi -i 1 -ac $(nvidia-smi -i 1 --query-supported-clocks=memory,graphics --format=csv,noheader,nounits | head -1 | tr "," " ") && nvidia-smi -i 0 -pl $(nvidia-smi -i 0 --query-gpu=power.max_limit --format=csv,noheader,nounits | head -1) && nvidia-smi -i 1 -pl $(nvidia-smi -i 1 --query-gpu=power.max_limit --format=csv,noheader,nounits | head -1) && echo "RTX 6000 Pro Max Q configuration complete!"'
RemainAfterExit=yes
User=root

[Install]
WantedBy=multi-user.target
EOF

echo "📊 Creating VRAM monitoring service..."
sudo cat > /etc/systemd/system/dual-vram-monitor.service << 'EOF'
[Unit]
Description=Dual RTX 6000 Pro Max Q VRAM Monitor for Trading
After=dual-gpu-trading-config.service
Requires=dual-gpu-trading-config.service

[Service]
Type=simple
ExecStart=/usr/bin/python3 /home/ChoubChoub/vram_monitor_dual_rtx6000.py
Restart=always
RestartSec=5
User=ChoubChoub
Environment=PYTHONPATH=/usr/local/lib/python3.11/site-packages
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF

echo ""
echo "🔄 Registering services with system..."
sudo systemctl daemon-reload

echo "✅ Enabling services to start automatically on boot..."
sudo systemctl enable dual-gpu-trading-config.service dual-vram-monitor.service

echo "🚀 Starting GPU configuration service now..."
sudo systemctl start dual-gpu-trading-config.service

echo ""
echo "📊 Checking service status..."
echo "GPU Configuration Service:"
sudo systemctl status dual-gpu-trading-config.service --no-pager -l

echo ""
echo "✅ Automatic startup services created!"
echo "📋 Services summary:"
echo "   - dual-gpu-trading-config.service: ✅ ENABLED"
echo "   - dual-vram-monitor.service: ✅ ENABLED"
echo ""
echo "💡 These services will:"
echo "   - Automatically configure your GPUs after every reboot"
echo "   - Start VRAM monitoring in the background"
echo "   - Restart monitoring if it crashes"
echo "   - Log all activity for troubleshooting"
```

## STEP 11: MEASURE OPTIMIZED PERFORMANCE AND COMPARE

**What we're doing:** Testing your GPUs again to see how much the optimization improved performance.
**Why this matters:** Proves the optimization worked and shows exact performance improvements.
**Time required:** 3-5 minutes
**What you'll see:** Detailed before/after comparison showing performance improvements.

```bash
# -----------------------------------------------------------------------------
# OPERATION: Post-Optimization Performance Measurement and Comparison
# WHY: Quantifies actual performance improvement to validate optimization effectiveness
# EXPECTED OUTCOME: Detailed before/after comparison showing <2ms variance achievement across both GPUs
# FAILURE MODE: If performance degraded, check thermal throttling and clock locking status
# RTX 6000 PRO MAX Q LIMITATION: Professional cards deliver superior consistency but require proper thermal management
# -----------------------------------------------------------------------------
echo "=== STEP 11: MEASURING OPTIMIZED PERFORMANCE ==="
echo "Testing your optimized RTX 6000 Pro Max Q cards and comparing to baseline..."
echo ""
echo "📋 This test will:"
echo "   - Run AI models on both GPUs with larger workloads"
echo "   - Measure timing consistency (our <2ms variance target)"
echo "   - Check thermal performance under sustained load"
echo "   - Compare results to your baseline measurements"
echo ""

cat > ~/performance_comparison_dual_rtx6000.py << 'EOF'
#!/usr/bin/env python3
import torch
import time
import statistics
import json
import subprocess
import os
from datetime import datetime

# Import and run deterministic configuration
exec(open(os.path.expanduser('~/pytorch_deterministic_config.py')).read())

def measure_optimized_dual_performance():
    print("🔧 Configuring PyTorch for testing...")
    configure_deterministic_pytorch()
    
    print("\n🧪 STARTING OPTIMIZED PERFORMANCE MEASUREMENT")
    print("=" * 60)
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'optimization_type': 'dual_rtx6000_pro_max_q_optimization',
        'system_info': {},
        'inference_timing': {},
        'thermal_profile': {},
        'vram_usage': {},
        'deterministic_config': {
            'cudnn_deterministic': torch.backends.cudnn.deterministic,
            'cudnn_benchmark': torch.backends.cudnn.benchmark,
            'use_deterministic_algorithms': True
        }
    }
    
    # Gather system information
    print("📋 Gathering system information...")
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        results['system_info']['gpu_count'] = gpu_count
        results['system_info']['cuda_version'] = torch.version.cuda
        results['system_info']['driver_version'] = subprocess.getoutput('nvidia-smi --query-gpu=driver_version --format=csv,noheader,nounits')
        
        # Get clock speeds for both GPUs
        for i in range(min(2, gpu_count)):
            clocks = subprocess.getoutput(f'nvidia-smi -i {i} --query-gpu=clocks.applications.graphics,clocks.applications.memory --format=csv,noheader,nounits')
            results['system_info'][f'gpu{i}_clocks'] = clocks
            gpu_name = torch.cuda.get_device_name(i)
            results['system_info'][f'gpu_{i}_name'] = gpu_name
            print(f"   GPU {i}: {gpu_name} - Clocks: {clocks}")
    
    # Create larger models to test 96GB VRAM capacity
    print("\n🏗️  Creating large AI models for testing...")
    device_0 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    device_1 = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cpu')
    
    # Larger models suitable for 96GB VRAM testing
    print("   Creating model for GPU 0...")
    model_0 = torch.nn.Sequential(
        torch.nn.Linear(8192, 16384),
        torch.nn.ReLU(),
        torch.nn.Linear(16384, 8192),
        torch.nn.ReLU(),
        torch.nn.Linear(8192, 4096)
    ).to(device_0)
    
    print("   Creating model for GPU 1...")
    model_1 = torch.nn.Sequential(
        torch.nn.Linear(8192, 16384),
        torch.nn.ReLU(),
        torch.nn.Linear(16384, 8192),
        torch.nn.ReLU(),
        torch.nn.Linear(8192, 4096)
    ).to(device_1)
    
    # Larger batch sizes for 96GB testing
    input_tensor_0 = torch.randn(256, 8192).to(device_0)
    input_tensor_1 = torch.randn(256, 8192).to(device_1)
    
    print("   Models created successfully!")
    
    # Extended warmup for stability
    print("\n🔥 Warming up GPUs for accurate measurements...")
    for i in range(200):
        if i % 50 == 0:
            print(f"   Warmup progress: {i}/200")
        _ = model_0(input_tensor_0)
        _ = model_1(input_tensor_1)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    print("   Warmup complete!")
    
    # Sustained performance measurement
    print("\n⏱️  Measuring sustained performance (this will take about 2 minutes)...")
    timings_gpu0 = []
    timings_gpu1 = []
    temps_gpu0 = []
    temps_gpu1 = []
    
    test_iterations = 2000
    
    for i in range(test_iterations):
        if i % 500 == 0:
            print(f"   Progress: {i}/{test_iterations} ({(i/test_iterations)*100:.0f}%)")
        
        # GPU 0 timing
        start = time.perf_counter()
        output_0 = model_0(input_tensor_0)
        if torch.cuda.is_available():
            torch.cuda.synchronize(device_0)
        end = time.perf_counter()
        timings_gpu0.append((end - start) * 1000)  # Convert to milliseconds
        
        # GPU 1 timing
        start = time.perf_counter()
        output_1 = model_1(input_tensor_1)
        if torch.cuda.is_available():
            torch.cuda.synchronize(device_1)
        end = time.perf_counter()
        timings_gpu1.append((end - start) * 1000)
        
        # Sample temperatures every 100 iterations
        if i % 100 == 0 and torch.cuda.is_available():
            temp_output = subprocess.getoutput('nvidia-smi --query-gpu=temperature.gpu --format=csv,noheader,nounits')
            temp_lines = temp_output.strip().split('\n')
            if len(temp_lines) >= 2:
                temps_gpu0.append(int(temp_lines[0]))
                temps_gpu1.append(int(temp_lines[1]))
    
    print("   Performance measurement complete!")
    
    # Calculate statistics for both GPUs
    print("\n📊 Calculating performance statistics...")
    for gpu_id, timings in [('gpu0', timings_gpu0), ('gpu1', timings_gpu1)]:
        mean_time = statistics.mean(timings)
        std_dev = statistics.stdev(timings)
        variance_percent = std_dev / mean_time * 100
        
        results['inference_timing'][gpu_id] = {
            'mean_ms': mean_time,
            'std_dev_ms': std_dev,
            'variance_percent': variance_percent,
            'min_ms': min(timings),
            'max_ms': max(timings),
            'p95_ms': sorted(timings)[int(0.95 * len(timings))],
            'p99_ms': sorted(timings)[int(0.99 * len(timings))],
            'target_achieved': std_dev < 2.0,
            'samples': len(timings)
        }
        
        print(f"   {gpu_id.upper()}: {mean_time:.3f}ms ± {std_dev:.3f}ms ({variance_percent:.2f}% variance)")
    
    # Thermal analysis
    print("\n🌡️  Analyzing thermal performance...")
    for gpu_id, temps in [('gpu0', temps_gpu0), ('gpu1', temps_gpu1)]:
        if temps:
            max_temp = max(temps)
            mean_temp = statistics.mean(temps)
            thermal_throttling = max_temp > 83  # RTX 6000 Pro Max Q threshold
            
            results['thermal_profile'][gpu_id] = {
                'max_temp_c': max_temp,
                'mean_temp_c': mean_temp,
                'thermal_throttling_detected': thermal_throttling,
                'temp_samples': len(temps)
            }
            
            print(f"   {gpu_id.upper()}: Max {max_temp}°C, Average {mean_temp:.1f}°C, Throttling: {'YES' if thermal_throttling else 'NO'}")
    
    # VRAM usage analysis
    print("\n💾 Analyzing VRAM usage...")
    if torch.cuda.is_available():
        total_vram = 0
        for i in range(min(2, torch.cuda.device_count())):
            torch.cuda.set_device(i)
            memory_info = torch.cuda.mem_get_info()
            gpu_vram = memory_info[1] / 1e9
            used_gb = (memory_info[1] - memory_info[0]) / 1e9
            total_vram += gpu_vram
            
            results['vram_usage'][f'gpu{i}'] = {
                'free_gb': memory_info[0] / 1e9,
                'total_gb': gpu_vram,
                'used_gb': used_gb,
                'used_percent': (1 - memory_info[0] / memory_info[1]) * 100,
                'expected_gb': 96.0
            }
            
            print(f"   GPU {i}: {used_gb:.1f}GB used / {gpu_vram:.1f}GB total")
        
        results['vram_usage']['total_system'] = {
            'total_gb': total_vram,
            'expected_gb': 192.0,
            'detection_accurate': total_vram > 190
        }
        
        print(f"   Total System: {total_vram:.1f}GB (Expected: 192GB)")
    
    # Save results
    results_file = os.path.expanduser('~/gpu_optimization_logs/optimized_performance_dual_rtx6000.json')
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\n💾 Results saved to: {results_file}")
    
    # Load and compare with baseline
    baseline_file = os.path.expanduser('~/gpu_optimization_logs/baseline_performance_dual_rtx6000.json')
    try:
        with open(baseline_file, 'r') as f:
            baseline = json.load(f)
        
        print("\n" + "=" * 80)
        print("🎯 OPTIMIZATION RESULTS COMPARISON")
        print("=" * 80)
        
        print(f"System: {results['system_info'].get('gpu_count', 0)} RTX 6000 Pro Max Q GPUs")
        print(f"Total VRAM: {results['vram_usage']['total_system']['total_gb']:.1f}GB")
        print(f"Driver Version: {results['system_info'].get('driver_version', 'Unknown')}")
        print(f"CUDA Version: {results['system_info'].get('cuda_version', 'Unknown')}")
        print("")
        
        # Performance comparison for both GPUs
        overall_improvement = 0
        gpus_meeting_target = 0
        
        for gpu_id in ['gpu0', 'gpu1']:
            if gpu_id in baseline.get('inference_timing', {}) and gpu_id in results['inference_timing']:
                baseline_gpu = baseline['inference_timing'][gpu_id]
                optimized_gpu = results['inference_timing'][gpu_id]
                
                baseline_std = baseline_gpu['std_dev_ms']
                optimized_std = optimized_gpu['std_dev_ms']
                improvement = ((baseline_std - optimized_std) / baseline_std * 100) if baseline_std > 0 else 0
                overall_improvement += improvement
                
                if optimized_std < 2.0:
                    gpus_meeting_target += 1
                
                print(f"{gpu_id.upper()} PERFORMANCE:")
                print(f"  Baseline:  {baseline_gpu['mean_ms']:.3f}ms ± {baseline_std:.3f}ms")
                print(f"  Optimized: {optimized_gpu['mean_ms']:.3f}ms ± {optimized_std:.3f}ms")
                print(f"  Improvement: {improvement:+.1f}% variance reduction")
                print(f"  Target (<2ms): {'✅ ACHIEVED' if optimized_std < 2.0 else '❌ NOT ACHIEVED'}")
                print("")
        
        # Overall summary
        avg_improvement = overall_improvement / 2 if overall_improvement else 0
        
        print("📊 OVERALL OPTIMIZATION SUMMARY:")
        print(f"  Average variance improvement: {avg_improvement:+.1f}%")
        print(f"  GPUs meeting <2ms target: {gpus_meeting_target}/2")
        print(f"  Deterministic mode: {'✅ ENABLED' if results['deterministic_config']['cudnn_deterministic'] else '❌ DISABLED'}")
        print(f"  VRAM detection: {'✅ ACCURATE' if results['vram_usage']['total_system']['detection_accurate'] else '❌ INACCURATE'}")
        
        # Thermal summary
        print(f"\n🌡️  THERMAL PERFORMANCE:")
        for gpu_id in ['gpu0', 'gpu1']:
            baseline_temp = baseline.get('thermal_profile', {}).get(gpu_id, {}).get('idle_temp_c', 0)
            max_temp = results['thermal_profile'].get(gpu_id, {}).get('max_temp_c', 0)
            throttling = results['thermal_profile'].get(gpu_id, {}).get('thermal_throttling_detected', False)
            
            print(f"  {gpu_id.upper()}: {baseline_temp}°C idle → {max_temp}°C load ({'⚠️ THROTTLING' if throttling else '✅ STABLE'})")
        
        if gpus_meeting_target == 2:
            print(f"\n🎉 OPTIMIZATION SUCCESS! Both GPUs achieved <2ms variance target.")
        elif gpus_meeting_target == 1:
            print(f"\n⚠️  PARTIAL SUCCESS: 1/2 GPUs achieved <2ms variance target.")
        else:
            print(f"\n❌ OPTIMIZATION INCOMPLETE: Neither GPU achieved <2ms variance target.")
            print("   Consider checking thermal throttling and clock locking status.")
        
    except FileNotFoundError:
        print(f"\n⚠️  Baseline file not found: {baseline_file}")
        print("Run the baseline measurement first to enable comparison.")
    
    return results

if __name__ == "__main__":
    measure_optimized_dual_performance()
EOF

chmod +x ~/performance_comparison_dual_rtx6000.py

echo "🚀 Running optimized performance measurement..."
python3 ~/performance_comparison_dual_rtx6000.py

echo ""
echo "✅ Performance measurement and comparison complete!"
echo "📁 Results saved to: ~/gpu_optimization_logs/optimized_performance_dual_rtx6000.json"
```

=============================================================================
## FINAL VALIDATION CHECKLIST

**What we're doing:** Running quick tests to make sure everything is working perfectly.
**Why this matters:** Confirms your RTX 6000 Pro Max Q cards are optimized and ready for trading.
**Time required:** 5 minutes total
**What you'll see:** A series of validation tests with pass/fail results.

### Test 1: Verify GPU Driver and CUDA Installation

```bash
echo "=== VALIDATION TEST 1: GPU DRIVER AND CUDA ==="
echo "Checking if NVIDIA drivers and CUDA are properly installed..."
echo ""

nvidia-smi && echo "✅ NVIDIA drivers working" || echo "❌ NVIDIA drivers failed"
nvcc --version && echo "✅ CUDA compiler working" ||
