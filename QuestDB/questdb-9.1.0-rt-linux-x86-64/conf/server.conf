################################################################################
# QuestDB Server Configuration - Crypto Trading Optimized
# Hardware: Samsung 990 Pro NVMe (3.6TB) + 192GB RAM + Intel Ultra 9 285K
# Purpose: 20-strategy crypto trading with 10+ exchanges, HFT components
# Performance Target: >200k inserts/sec, <100ms query latency
# Created: 2025-10-18
################################################################################

################################################################################
# STORAGE CONFIGURATION - Hot/Cold Tiering Strategy
################################################################################

# Hot Storage: NVMe for active trading data (30 days tick, 7 days orderbook)
# Location: /home/youssefbahloul/ai-trading-station/QuestDB/data/hot
cairo.root=/home/youssefbahloul/ai-trading-station/QuestDB/data/hot

# Cold Storage: HDD for historical data (365+ days) - TO BE CONFIGURED AFTER HDD MOUNT
# Uncomment after mounting /dev/sda to /mnt/hdd:
# cairo.cold.storage.root=/mnt/hdd/questdb/cold

################################################################################
# MEMORY CONFIGURATION - Optimized for 192GB RAM System
################################################################################

# Memory limit: 96GB (50% of 192GB total RAM)
# Allocation breakdown:
# - QuestDB: 96GB (this setting)
# - Redis: 8GB (configured in redis-hft.conf)
# - GPU/CUDA: 40GB
# - OS/Services: 20GB
# - Page cache/Buffer: 28GB
cairo.memory.limit=103079215104  # 96GB in bytes

# Memory page frame size optimized for Samsung 990 Pro (4KB native page size)
cairo.page.size=4096

# Memory mapped file size for large datasets
cairo.max.file.size=1073741824  # 1GB per file

################################################################################
# WRITE-AHEAD LOG (WAL) - NVMe Sequential Write Optimization
################################################################################

# Enable WAL for data durability (critical for trading)
cairo.wal.enabled=true

# WAL segment size: 256MB optimized for NVMe sequential writes (7,350MB/s)
cairo.wal.max.segment.size=268435456

# WAL writer buffer: 8MB (matches NVMe optimal I/O size)
cairo.wal.writer.buffer.size=8388608

# Commit lag: 1 second (from Crypto-Backtesting-Engine-Final.md)
cairo.commit.lag=1000

# Max uncommitted rows: 500K (from documentation)
cairo.max.uncommitted.rows=500000

################################################################################
# WORKER THREAD CONFIGURATION - Scaled for 20 Strategies + 10 Exchanges
################################################################################

# HTTP worker threads: 20 (one per strategy for parallel processing)
# Intel Ultra 9 285K has 8 P-cores, but with hyperthreading = 16 logical cores
# Plus E-cores disabled = clean 8 P-cores @ 5.7GHz
# Setting to 20 allows over-subscription for I/O-bound operations
http.worker.count=20

# Line protocol TCP I/O workers: 8 (handles 10+ exchange WebSocket feeds)
# Increased from 4 to handle high-frequency tick ingestion
line.tcp.io.worker.count=8

# Shared worker threads: 16 (for query parallelism across strategies)
shared.worker.count=16

# HTTP worker affinity: distribute across P-cores (cores 2-7, trading cores)
# Cores 0-1 reserved for housekeeping (IRQ affinity)
http.worker.affinity=2,3,4,5,6,7,2,3,4,5,6,7,2,3,4,5,6,7,2,3

################################################################################
# CONNECTION POOLING - From Documentation Specifications
################################################################################

# Initial connection pool size (from docs: 5 min)
http.connection.pool.initial.size=5

# Max connection pool size (from docs: 50 max)
http.connection.pool.max.size=50

# Connection timeout: 5 seconds
http.connection.timeout=5000

# Idle connection timeout: 5 minutes
http.connection.idle.timeout=300000

################################################################################
# NETWORK BUFFER CONFIGURATION - 10+ Exchange High-Throughput Ingestion
################################################################################

# Line protocol TCP receive buffer: 16MB per connection
# Handles burst tick data from multiple exchanges simultaneously
line.tcp.recv.buf=16777216

# HTTP receive buffer: 4MB (increased for large REST API responses)
http.recv.buf=4194304

# HTTP send buffer: 2MB (optimized for query results)
http.send.buf=2097152

# Line protocol TCP connection timeout
line.tcp.connection.timeout=300000

################################################################################
# INDEXING OPTIMIZATION - Multi-Strategy Random Access Patterns
################################################################################

# Enable parallel indexing for NVMe concurrent I/O
cairo.parallel.indexing=true

# Timestamp index key cache: 128K entries (doubled for 20 strategies)
# Each strategy needs fast timestamp lookups across multiple exchanges
cairo.timestamp.key.cache.size=131072

# Writer data index key count: 128K (doubled for multi-exchange writes)
cairo.writer.data.index.key.count=131072

# Spin lock timeout for concurrent access: 1 second
cairo.spin.lock.timeout=1000

################################################################################
# COMPRESSION - From Crypto-Backtesting-Engine-Final.md
################################################################################

# Default compression: LZ4 (fast compression for hot data)
# ZSTD will be used for cold storage archival
cairo.default.compression=LZ4

# Column compression threshold
cairo.compression.column.size.threshold=8388608  # 8MB

################################################################################
# TIMESTAMP CONFIGURATION - Microsecond Precision for HFT
################################################################################

# Timestamp precision: microseconds (CRITICAL for HFT components)
# Required for <100μs tick processing and regime detection
cairo.timestamp.precision=microsecond

################################################################################
# QUERY OPTIMIZATION - Fast Data Access for 20 Parallel Strategies
################################################################################

# Query cache size: 512 entries (increased for multi-strategy queries)
cairo.query.cache.size=512

# SQL page frame size: 4MB (matches NVMe optimal transfer)
cairo.sql.page.frame.size=4194304

# Max query memory: 10GB per query (from docs)
cairo.max.query.memory=10737418240

# Global memory limit for all queries: 80GB (from docs, adjusted for 96GB total)
cairo.global.memory.limit=85899345920

################################################################################
# PARTITION MANAGEMENT - From Documentation Table Configs
################################################################################

# Default partition by: DAY (for tick data as per docs)
# Allows efficient hot/cold data lifecycle management
cairo.default.partition.by=DAY

# Partition purge: automatic cleanup of old partitions
cairo.partition.purge.enabled=true

# O3 max lag: 10 seconds (out-of-order data acceptance window)
cairo.o3.max.lag=10000000  # 10 seconds in microseconds

################################################################################
# HTTP SERVER CONFIGURATION
################################################################################

# HTTP server enabled
http.enabled=true

# Bind address: localhost only (secure default)
http.bind.to=0.0.0.0

# HTTP port
http.port=9000

# Min HTTP threads: 4
http.min.threads=4

# Max HTTP threads: 16
http.max.threads=16

# HTTP connection pool timeout
http.connection.pool.timeout=60000

################################################################################
# LINE PROTOCOL TCP (InfluxDB format)
################################################################################

# Line protocol enabled for high-frequency tick ingestion
line.tcp.enabled=true

# Bind address
line.tcp.bind.to=0.0.0.0

# Line protocol port
line.tcp.port=9009

# Parallel writers: 4 (from Redis→QuestDB pipeline docs)
line.tcp.writer.count=4

# Line protocol buffer size: 8MB
line.tcp.net.buffer.size=8388608

################################################################################
# POSTGRES WIRE PROTOCOL (for SQL clients)
################################################################################

# PostgreSQL wire protocol enabled
pg.enabled=true

# Bind address
pg.bind.to=0.0.0.0

# PostgreSQL port
pg.port=8812

# Max PostgreSQL connections
pg.max.connections=50

################################################################################
# TELEMETRY & MONITORING
################################################################################

# Telemetry disabled (no data collection)
telemetry.enabled=false

# Metrics enabled for monitoring
metrics.enabled=true

################################################################################
# PERFORMANCE TUNING - NVMe-Specific Optimizations
################################################################################

# File operation retry count (for busy NVMe)
cairo.file.operation.retry.count=10

# Writer data append page size: 4MB (NVMe optimal)
cairo.writer.data.append.page.size=4194304

# Reader pool max segments: 1024 (high parallelism)
cairo.reader.pool.max.segments=1024

# Parallel import: enabled for bulk data loads
cairo.parallel.import.enabled=true

# Max parallel import threads: 8
cairo.max.parallel.import.threads=8

################################################################################
# SECURITY CONFIGURATION
################################################################################

# Authentication: disabled for local development
# ENABLE FOR PRODUCTION DEPLOYMENT
http.security.enabled=false

# Read-only mode: disabled (allow writes)
cairo.read.only.enabled=false

################################################################################
# LOGGING CONFIGURATION
################################################################################

# Log level: INFO for production monitoring
log.level=INFO

# Log to stdout
log.stdout=true

# Log file rotation
log.file.rotation.enabled=true

################################################################################
# DATA LIFECYCLE MANAGEMENT - Hot/Cold Tiering (Future Implementation)
################################################################################

# Data retention policies (to be enforced in application layer):
# - crypto_ticks: Hot 30 days (NVMe) → Cold 365 days (HDD) → Archive
# - orderbook_snapshots: Hot 7 days → Cold 90 days → Delete
# - regime_states: Hot 30 days → Cold 365 days → Archive

# Automatic partition archival (to be configured after HDD mount)
# cairo.partition.archive.enabled=true
# cairo.partition.archive.path=/mnt/hdd/questdb/cold

################################################################################
# NOTES FOR PRODUCTION DEPLOYMENT
################################################################################

# TODO before production:
# 1. Mount /dev/sda (7.3TB HDD) to /mnt/hdd
# 2. Enable cairo.cold.storage.root for historical data archival
# 3. Configure partition lifecycle policies
# 4. Enable HTTP authentication (http.security.enabled=true)
# 5. Set up monitoring alerts for:
#    - Insert rate >200k/sec validation
#    - Query latency <100ms validation
#    - NVMe utilization <80%
#    - Memory usage <90%
# 6. Configure backup strategy for /dev/sda
# 7. Set up QuestDB systemd service for auto-restart
# 8. Implement graceful shutdown handling for trading hours
# 9. Configure log rotation and archival

################################################################################
# PERFORMANCE VALIDATION TARGETS (from Documentation)
################################################################################

# Expected Performance with this Configuration:
# - Tick Processing: >200,000 inserts/second
# - Query Latency: <100ms (p99)
# - Write Throughput: ~7GB/s (NVMe bandwidth)
# - Concurrent Strategies: 20 parallel
# - Exchange Feeds: 10+ simultaneous
# - Memory Efficiency: <90% of 96GB allocation
# - NVMe Utilization: 60-80% during trading hours

################################################################################
# INTEGRATION NOTES
################################################################################

# Redis Integration:
# - Redis acts as tick buffer (8GB, configured in redis-hft.conf)
# - QuestDB persists for long-term storage and backtesting
# - Pipeline: Exchange → Redis Stream → QuestDB (4 parallel workers)
# - Latency: <10ms Redis→QuestDB insertion

# GPU Integration:
# - QuestDB provides historical data for GPU backtesting
# - Query optimization critical for GPU data loading
# - Fast NVMe access prevents GPU starvation during training

# Multi-Strategy Architecture:
# - 20 strategies share QuestDB connection pool (max 50 connections)
# - Each strategy gets dedicated HTTP worker thread
# - Parallel queries enabled for simultaneous strategy backtests
# - Timestamp index critical for strategy correlation analysis

################################################################################
# END OF CONFIGURATION
################################################################################
