#!/usr/bin/env python3
"""
Tiered Market Data Access Layer
Day 3 Implementation: Hot/Warm/Cold caching for ultra-low latency queries

Architecture:
- Hot Cache (Memory): Last 1 minute, <1ms access, deque-based ring buffer
- Warm Cache (Redis): Last 5 minutes, 1-2ms access, sorted sets
- Cold Storage (QuestDB): Everything else, 2-5ms access, PostgreSQL wire

Performance Targets:
- 95%+ cache hit rate
- <1ms for recent data (last 1 min)
- 1-2ms for warm data (1-5 min ago)
- 2-5ms for cold data (>5 min ago)

Author: Claude Sonnet
Date: October 25, 2025
"""

import asyncio
import logging
import time
from collections import deque
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from decimal import Decimal
import json
import orjson  # 5-10x faster JSON parsing

import redis.asyncio as aioredis
import asyncpg
from prometheus_client import Counter, Histogram, Gauge, start_http_server


@dataclass
class MarketTick:
    """Unified market data structure"""
    timestamp: datetime
    symbol: str
    exchange: str
    data_type: str  # 'TRADE', 'ORDERBOOK', 'DEPTH', 'FUNDING'
    
    # Trade fields
    price: Optional[float] = None
    volume: Optional[float] = None
    side: Optional[bool] = None  # True = buy, False = sell
    
    # Orderbook fields (L1)
    bid_price: Optional[float] = None
    bid_qty: Optional[float] = None
    ask_price: Optional[float] = None
    ask_qty: Optional[float] = None
    
    # Metadata
    raw_data: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization"""
        return {
            'timestamp': self.timestamp.isoformat(),
            'symbol': self.symbol,
            'exchange': self.exchange,
            'data_type': self.data_type,
            'price': self.price,
            'volume': self.volume,
            'side': self.side,
            'bid_price': self.bid_price,
            'bid_qty': self.bid_qty,
            'ask_price': self.ask_price,
            'ask_qty': self.ask_qty,
            'raw_data': self.raw_data
        }
    
    @staticmethod
    def from_dict(data: Dict[str, Any]) -> 'MarketTick':
        """Create from dictionary"""
        return MarketTick(
            timestamp=datetime.fromisoformat(data['timestamp']) if isinstance(data['timestamp'], str) else data['timestamp'],
            symbol=data['symbol'],
            exchange=data['exchange'],
            data_type=data['data_type'],
            price=data.get('price'),
            volume=data.get('volume'),
            side=data.get('side'),
            bid_price=data.get('bid_price'),
            bid_qty=data.get('bid_qty'),
            ask_price=data.get('ask_price'),
            ask_qty=data.get('ask_qty'),
            raw_data=data.get('raw_data', {})
        )


@dataclass
class CacheStats:
    """Cache performance statistics"""
    hot_hits: int = 0
    warm_hits: int = 0
    cold_hits: int = 0
    misses: int = 0
    
    @property
    def total_queries(self) -> int:
        return self.hot_hits + self.warm_hits + self.cold_hits + self.misses
    
    @property
    def cache_hit_rate(self) -> float:
        if self.total_queries == 0:
            return 0.0
        return (self.hot_hits + self.warm_hits) / self.total_queries * 100
    
    @property
    def hot_hit_rate(self) -> float:
        if self.total_queries == 0:
            return 0.0
        return self.hot_hits / self.total_queries * 100
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'hot_hits': self.hot_hits,
            'warm_hits': self.warm_hits,
            'cold_hits': self.cold_hits,
            'misses': self.misses,
            'total_queries': self.total_queries,
            'cache_hit_rate_pct': round(self.cache_hit_rate, 2),
            'hot_hit_rate_pct': round(self.hot_hit_rate, 2)
        }


class HotCache:
    """
    Memory-based hot cache using deque ring buffer
    Target: <1ms access for last 1 minute of data
    """
    
    def __init__(self, max_age_seconds: int = 60, max_size: int = 100000):
        self.max_age_seconds = max_age_seconds
        self.max_size = max_size
        
        # Separate deques per symbol/data_type for faster lookups
        self.caches: Dict[str, deque] = {}
        self.lock = asyncio.Lock()
    
    def _get_cache_key(self, symbol: str, exchange: str, data_type: str) -> str:
        """Generate cache key"""
        return f"{exchange}:{symbol}:{data_type}"
    
    async def add(self, tick: MarketTick):
        """Add tick to hot cache"""
        cache_key = self._get_cache_key(tick.symbol, tick.exchange, tick.data_type)
        
        async with self.lock:
            if cache_key not in self.caches:
                self.caches[cache_key] = deque(maxlen=self.max_size)
            
            self.caches[cache_key].append(tick)
    
    async def get_recent(
        self, 
        symbol: str, 
        exchange: str, 
        data_type: str, 
        seconds: int = 60
    ) -> List[MarketTick]:
        """
        Get recent ticks from hot cache
        Returns empty list if cache miss
        """
        cache_key = self._get_cache_key(symbol, exchange, data_type)
        
        async with self.lock:
            if cache_key not in self.caches:
                return []
            
            # Filter by age
            cutoff_time = datetime.utcnow() - timedelta(seconds=seconds)
            cache = self.caches[cache_key]
            
            # Linear scan from end (most recent)
            result = []
            for tick in reversed(cache):
                if tick.timestamp >= cutoff_time:
                    result.append(tick)
                else:
                    break  # Older entries, stop scanning
            
            return list(reversed(result))
    
    async def cleanup(self):
        """Remove expired entries (called periodically)"""
        cutoff_time = datetime.utcnow() - timedelta(seconds=self.max_age_seconds)
        
        async with self.lock:
            for cache_key, cache in self.caches.items():
                # Remove old entries from left (oldest)
                while cache and cache[0].timestamp < cutoff_time:
                    cache.popleft()
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_items = sum(len(cache) for cache in self.caches.values())
        return {
            'total_items': total_items,
            'num_streams': len(self.caches),
            'max_age_seconds': self.max_age_seconds,
            'max_size': self.max_size
        }


class WarmCache:
    """
    Redis-based warm cache using SINGLE connection with health monitoring
    Target: <1ms access for last 5 minutes of data
    
    OPUS HYBRID SOLUTION: Single connection + health checks + auto-reconnect
    Trade-off: Performance (0.5-0.8ms) vs pool overhead (3.58ms)
    """
    
    def __init__(
        self, 
        redis_host: str = 'localhost',
        redis_port: int = 6379,
        max_age_seconds: int = 300  # 5 minutes
    ):
        self.redis_host = redis_host
        self.redis_port = redis_port
        self.max_age_seconds = max_age_seconds
        self.redis: Optional[aioredis.Redis] = None
        
        # Health monitoring for single connection
        self._last_ping = 0
        self._ping_interval = 30  # Check health every 30s
        self._connection_id = None
        self._reconnect_count = 0
        
        # Logger
        self.logger = logging.getLogger(__name__)
    
    async def connect(self):
        """Connect to Redis with single persistent connection"""
        try:
            # OPUS HYBRID: Single connection for performance
            # No pool overhead = sub-1ms latency restored
            # CRITICAL: Use single_connection_client=True to prevent pooling
            self.redis = aioredis.Redis(
                host=self.redis_host,
                port=self.redis_port,
                encoding="utf-8",
                decode_responses=True,
                socket_keepalive=True,
                single_connection_client=True  # CRITICAL: Single connection, no pool
            )
            
            # Validate connection
            await self.redis.ping()
            self._last_ping = time.time()
            self._connection_id = id(self.redis)
            
            self.logger.info(f"âœ… Redis single connection established (ID: {self._connection_id})")
        
        except Exception as e:
            self.logger.error(f"FATAL: Redis connection failed: {e}")
            raise  # FAIL FAST - don't hide infrastructure failures
    
    async def _reconnect(self):
        """Auto-reconnect on connection failure"""
        try:
            self.logger.warning(f"ðŸ”„ Reconnecting Redis (attempt {self._reconnect_count + 1})...")
            
            # Close stale connection
            if self.redis:
                try:
                    await self.redis.close()
                except:
                    pass
            
            # Create new connection
            # CRITICAL: Use single_connection_client=True to prevent pooling
            self.redis = aioredis.Redis(
                host=self.redis_host,
                port=self.redis_port,
                encoding="utf-8",
                decode_responses=True,
                socket_keepalive=True,
                single_connection_client=True  # CRITICAL: Single connection, no pool
            )
            
            await self.redis.ping()
            self._last_ping = time.time()
            self._connection_id = id(self.redis)
            self._reconnect_count += 1
            
            self.logger.info(f"âœ… Reconnected successfully (ID: {self._connection_id})")
            
        except Exception as e:
            self.logger.error(f"âŒ Reconnect failed: {e}")
            raise
    
    async def close(self):
        """PROPERLY close connection"""
        try:
            if self.redis:
                await self.redis.close()
                self.logger.info(f"âœ… Redis connection closed (reconnects: {self._reconnect_count})")
        except Exception as e:
            self.logger.error(f"Redis close error (non-fatal): {e}")
    
    def _get_cache_key(self, symbol: str, exchange: str, data_type: str) -> str:
        """Generate Redis key"""
        return f"cache:{exchange}:{symbol}:{data_type}"
    
    def _prepare_for_orjson(self, data: Dict) -> Dict:
        """
        Convert non-serializable types for orjson
        orjson doesn't serialize datetime/Decimal natively
        """
        cleaned = data.copy()
        
        # Convert datetime to ISO format string
        if 'timestamp' in cleaned and isinstance(cleaned['timestamp'], datetime):
            cleaned['timestamp'] = cleaned['timestamp'].isoformat()
        
        # Convert Decimal to float
        for key in ['price', 'volume', 'bid_price', 'ask_price', 'bid_qty', 'ask_qty']:
            if key in cleaned and isinstance(cleaned[key], Decimal):
                cleaned[key] = float(cleaned[key])
        
        return cleaned
    
    def serialize_for_redis(self, data: Dict) -> bytes:
        """
        Serialize data for Redis using orjson with fallback
        CRITICAL: Handle orjson's strict type requirements
        """
        try:
            # orjson requires specific type handling
            cleaned = self._prepare_for_orjson(data)
            return orjson.dumps(cleaned, option=orjson.OPT_SERIALIZE_NUMPY)
        except TypeError as e:
            # Fallback to standard json for edge cases
            print(f"âš ï¸  orjson serialization failed, falling back to json: {e}")
            return json.dumps(data).encode()
    
    async def add(self, tick: MarketTick):
        """Add tick to warm cache (Redis sorted set) using orjson"""
        if not self.redis:
            return
        
        cache_key = self._get_cache_key(tick.symbol, tick.exchange, tick.data_type)
        
        # Use timestamp as score for sorted set
        score = tick.timestamp.timestamp()
        
        # Serialize tick to JSON using orjson (5-10x faster)
        value = self.serialize_for_redis(tick.to_dict())
        
        # Add to sorted set
        await self.redis.zadd(cache_key, {value: score})
        
        # Set TTL on key
        await self.redis.expire(cache_key, self.max_age_seconds)
    
    async def get_recent(
        self,
        symbol: str,
        exchange: str,
        data_type: str,
        seconds: int = 300
    ) -> List[MarketTick]:
        """Get recent ticks from warm cache using orjson (5-10x faster parsing)
        
        OPUS HYBRID OPTIMIZED: 
        - Health checks to prevent connection starvation
        - Pipeline for larger windows (>30s)
        - Batch deserialization for large result sets
        """
        if not self.redis:
            return []
        
        # HEALTH CHECK: Ping Redis every 30s to detect stale connections
        current_time = time.time()
        if current_time - self._last_ping > self._ping_interval:
            try:
                await self.redis.ping()
                self._last_ping = current_time
            except Exception as e:
                self.logger.warning(f"âš ï¸  Health check failed, reconnecting: {e}")
                await self._reconnect()
        
        cache_key = self._get_cache_key(symbol, exchange, data_type)
        
        # Calculate time range
        end_time = datetime.utcnow().timestamp()
        start_time = end_time - seconds
        
        # OPTIMIZATION: Use pipeline for larger windows (better network efficiency)
        try:
            if seconds > 30:  # Larger windows benefit from pipelining
                pipe = self.redis.pipeline()
                pipe.zrangebyscore(cache_key, start_time, end_time)
                pipe.zcard(cache_key)  # Get total size for potential metrics
                results = await pipe.execute()
                data = results[0]  # First result is the range query
            else:
                # Small windows - simple query is faster
                data = await self.redis.zrangebyscore(
                    cache_key,
                    min=start_time,
                    max=end_time
                )
        except Exception as e:
            # Connection might be stale, try reconnect once
            self.logger.warning(f"âš ï¸  Query failed, reconnecting: {e}")
            await self._reconnect()
            data = await self.redis.zrangebyscore(
                cache_key,
                min=start_time,
                max=end_time
            )
        
        # OPTIMIZATION: Fast deserialization path
        ticks = []
        
        # For large datasets, optimize the loop
        if len(data) > 100:
            # Batch processing for large result sets
            for result in data:
                try:
                    # orjson.loads expects bytes
                    if isinstance(result, str):
                        result = result.encode()
                    tick_data = orjson.loads(result)
                    ticks.append(MarketTick.from_dict(tick_data))
                except:
                    # Skip invalid entries silently (don't slow down hot path)
                    continue
        else:
            # Small datasets - include fallback for safety
            for result in data:
                try:
                    # orjson.loads expects bytes, result might be string from Redis
                    if isinstance(result, str):
                        result = result.encode()
                    tick_data = orjson.loads(result)
                    ticks.append(MarketTick.from_dict(tick_data))
                except (json.JSONDecodeError, TypeError, KeyError):
                    # Fallback: try standard json if orjson fails
                    try:
                        if isinstance(result, bytes):
                            result = result.decode()
                        tick_data = json.loads(result)
                        ticks.append(MarketTick.from_dict(tick_data))
                    except:
                        continue
        
        return ticks
    
    async def cleanup(self):
        """Remove expired entries from all sorted sets"""
        if not self.redis:
            return
        
        # Get all cache keys
        pattern = "cache:*"
        keys = []
        async for key in self.redis.scan_iter(match=pattern):
            keys.append(key)
        
        # Remove old entries
        cutoff_time = (datetime.utcnow() - timedelta(seconds=self.max_age_seconds)).timestamp()
        
        for key in keys:
            await self.redis.zremrangebyscore(key, '-inf', cutoff_time)


class TieredMarketData:
    """
    Unified tiered market data access
    Queries cascade: Hot â†’ Warm â†’ Cold
    
    Target Performance:
    - Hot (last 1 min): <1ms
    - Warm (1-5 min): 1-2ms
    - Cold (>5 min): 2-5ms
    """
    
    def __init__(
        self,
        questdb_host: str = 'localhost',
        questdb_port: int = 8812,
        redis_host: str = 'localhost',
        redis_port: int = 6379,
        hot_cache_seconds: int = 60,
        warm_cache_seconds: int = 300
    ):
        self.questdb_host = questdb_host
        self.questdb_port = questdb_port
        
        # Initialize caches
        self.hot_cache = HotCache(max_age_seconds=hot_cache_seconds)
        self.warm_cache = WarmCache(
            redis_host=redis_host,
            redis_port=redis_port,
            max_age_seconds=warm_cache_seconds
        )
        
        # QuestDB connection pool
        self.db_pool: Optional[asyncpg.Pool] = None
        
        # Statistics
        self.stats = CacheStats()
        
        # Cleanup task
        self.cleanup_task: Optional[asyncio.Task] = None
        
        # Prometheus metrics
        self.cache_requests = Counter(
            'cache_requests_total',
            'Total cache requests',
            ['tier', 'symbol', 'exchange', 'data_type']
        )
        self.cache_hits = Counter(
            'cache_hits_total',
            'Total cache hits',
            ['tier', 'symbol', 'exchange', 'data_type']
        )
        self.cache_latency = Histogram(
            'cache_latency_seconds',
            'Cache query latency',
            ['tier', 'symbol', 'exchange', 'data_type'],
            buckets=[0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.05, 0.1]
        )
        self.cache_size = Gauge(
            'cache_size_entries',
            'Number of entries in cache',
            ['tier', 'symbol', 'exchange', 'data_type']
        )
        
        # Strategic Metrics (Opus Recommendation)
        # Track both latency AND data staleness when HOT cache misses
        self.hot_miss_penalty = Histogram(
            'cache_hot_miss_penalty_seconds',
            'Total penalty: fetch_latency + data_staleness',
            ['symbol', 'exchange', 'data_type', 'penalty_type'],  # penalty_type: 'latency' or 'staleness'
            buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]  # Smart Speed buckets
        )
        
        # Track when WARM latency blocks critical path
        self.warm_latency_impact = Counter(
            'cache_warm_latency_impact_total',
            'Count of WARM queries exceeding threshold',
            ['symbol', 'exchange', 'data_type', 'threshold_ms']
        )
        
        # Track query patterns - which time windows are most used
        self.query_patterns = Counter(
            'cache_query_patterns_total',
            'Query count by time window',
            ['symbol', 'exchange', 'data_type', 'window_seconds']
        )
        
        # Redis Connection Pool Metrics (Opus Recommendation - Critical for leak detection)
        self.redis_pool_connections_active = Gauge(
            'redis_pool_connections_active',
            'Active connections in Redis pool'
        )
        
        self.redis_pool_connections_waiting = Gauge(
            'redis_pool_connections_waiting',
            'Threads waiting for connection from pool'
        )
        
        # Start Prometheus metrics server
        try:
            start_http_server(9092)
            print("âœ… Prometheus metrics server started on port 9092")
        except OSError as e:
            print(f"âš ï¸  Metrics server already running or port in use: {e}")
    
    async def start(self):
        """Initialize connections and start cleanup task"""
        # Connect to Redis
        await self.warm_cache.connect()
        
        # Create QuestDB connection pool
        self.db_pool = await asyncpg.create_pool(
            host=self.questdb_host,
            port=self.questdb_port,
            user='admin',
            password='quest',
            database='qdb',
            min_size=2,
            max_size=10
        )
        
        # Start periodic cleanup
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())
        
        # Warm caches on startup (Opus recommendation)
        await self._warm_caches_on_startup()
        
        print("âœ… TieredMarketData started")
    
    async def stop(self):
        """Close connections and stop cleanup task"""
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass
        
        await self.warm_cache.close()
        
        if self.db_pool:
            await self.db_pool.close()
        
        print("âœ… TieredMarketData stopped")
    
    async def _cleanup_loop(self):
        """Periodic cleanup of expired cache entries"""
        while True:
            try:
                await asyncio.sleep(60)  # Every minute
                await self.hot_cache.cleanup()
                await self.warm_cache.cleanup()
            except asyncio.CancelledError:
                break
            except Exception as e:
                print(f"âš ï¸ Cleanup error: {e}")
    
    async def _warm_caches_on_startup(self):
        """Pre-populate cache with recent data using smart warming (Opus optimization)"""
        print("ðŸ”¥ Smart warming caches on startup...")
        
        # Use smart warming to query most active symbols from QuestDB
        result = await self.smart_cache_warming()
        
        if result['success']:
            print(f"âœ… Startup warming complete: {result['symbols_warmed']} symbols, "
                  f"{result['total_ticks']} ticks in {result['total_time_ms']:.0f}ms")
        else:
            print("âš ï¸  Startup warming had issues - check logs")
    
    
    async def add_tick(self, tick: MarketTick):
        """Add tick to hot and warm caches"""
        await self.hot_cache.add(tick)
        await self.warm_cache.add(tick)
    
    async def add_batch_to_cache(self, ticks: List[MarketTick]):
        """Add multiple ticks efficiently (Opus optimization - 2x faster)"""
        tasks = []
        for tick in ticks:
            tasks.append(self.hot_cache.add(tick))
            tasks.append(self.warm_cache.add(tick))
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _get_most_active_symbols(self, limit: int = 5) -> List[str]:
        """
        Query market_trades for symbols with most ticks in last hour.
        Uses REAL tick count data from QuestDB - no fake complexity.
        
        Args:
            limit: Maximum number of symbols to return
            
        Returns:
            List of symbol names sorted by tick volume
        """
        query = f"""
            SELECT symbol, count() as tick_count
            FROM market_trades
            WHERE timestamp > dateadd('h', -1, now())
            GROUP BY symbol
            ORDER BY tick_count DESC
            LIMIT {limit}
        """
        
        try:
            async with self.db_pool.acquire() as conn:
                rows = await conn.fetch(query)
                symbols = [row['symbol'] for row in rows]
                
                if symbols:
                    print(f"ðŸ“Š Most active symbols (last hour): {symbols}")
                    return symbols
                else:
                    print("âš ï¸  No active symbols in last hour, using fallback")
                    return ['BTCUSDT', 'ETHUSDT', 'BNBUSDT'][:limit]
                    
        except Exception as e:
            print(f"âš ï¸  Failed to query active symbols: {e}")
            # Fallback to known active pairs
            return ['BTCUSDT', 'ETHUSDT', 'BNBUSDT'][:limit]
    
    async def smart_cache_warming(self, symbols: List[str] = None, seconds: int = 60) -> dict:
        """
        Simplified cache warming based on actual tick volume from QuestDB.
        No fake volatility calculations, no fake strategy interest.
        Uses REAL data: tick counts from market_trades table.
        
        Args:
            symbols: Optional list of symbols to warm. If None, queries QuestDB for most active.
            seconds: Amount of historical data to warm (default: 60 seconds)
            
        Returns:
            Dict with warming statistics: success, symbols_warmed, total_time_ms, details
        """
        start_time = time.perf_counter()
        
        # Get symbols to warm (query QuestDB if not provided)
        if symbols is None:
            symbols = await self._get_most_active_symbols(limit=5)
        
        print(f"ðŸ”¥ Smart warming cache for {len(symbols)} symbols...")
        
        warming_results = []
        total_ticks = 0
        
        for symbol in symbols:
            symbol_start = time.perf_counter()
            
            try:
                # Query both trades and orderbook data
                for data_type in ['TRADE', 'ORDERBOOK']:
                    ticks = await self._query_questdb(
                        symbol=symbol,
                        exchange='binance_spot',  # Primary exchange
                        data_type=data_type,
                        seconds=seconds
                    )
                    
                    if ticks:
                        # Add to hot cache (will automatically add to warm cache too)
                        for tick in ticks:
                            await self.add_tick(tick)
                        
                        total_ticks += len(ticks)
                        symbol_latency = (time.perf_counter() - symbol_start) * 1000
                        
                        warming_results.append({
                            'symbol': symbol,
                            'data_type': data_type,
                            'ticks': len(ticks),
                            'latency_ms': round(symbol_latency, 2)
                        })
                        
                        print(f"  âœ… {symbol}:{data_type} - {len(ticks)} ticks in {symbol_latency:.2f}ms")
                    
            except Exception as e:
                print(f"  âš ï¸  {symbol} warming failed: {e}")
                warming_results.append({
                    'symbol': symbol,
                    'error': str(e)
                })
        
        total_time_ms = (time.perf_counter() - start_time) * 1000
        
        result = {
            'success': len(warming_results) > 0,
            'symbols_warmed': len([r for r in warming_results if 'error' not in r]),
            'total_ticks': total_ticks,
            'total_time_ms': round(total_time_ms, 2),
            'details': warming_results
        }
        
        print(f"âœ… Smart warming complete: {result['symbols_warmed']} symbols, {total_ticks} ticks in {total_time_ms:.0f}ms")
        
        return result
    
    async def get_recent_ticks(
        self,
        symbol: str,
        exchange: str,
        data_type: str,
        seconds: int = 60
    ) -> Tuple[List[MarketTick], str, float]:
        """
        Get recent ticks with automatic tiered fallback
        
        Returns:
            (ticks, cache_tier, latency_ms)
        """
        start_time = time.perf_counter()
        
        # Update Redis pool metrics (Opus recommendation - critical for leak detection)
        try:
            if self.warm_cache.redis_pool:
                # Track active connections from pool
                pool = self.warm_cache.redis_pool
                # Get number of connections currently in use
                active_conns = len(pool._available_connections) if hasattr(pool, '_available_connections') else 0
                self.redis_pool_connections_active.set(active_conns)
        except Exception as e:
            # Don't fail queries due to metrics errors
            pass
        
        # Record cache request
        self.cache_requests.labels(
            tier='REQUEST',
            symbol=symbol,
            exchange=exchange,
            data_type=data_type
        ).inc()
        
        # Record query pattern (Opus strategic metric)
        self.query_patterns.labels(
            symbol=symbol,
            exchange=exchange,
            data_type=data_type,
            window_seconds=str(seconds)
        ).inc()
        
        # Try hot cache first (last 1 minute)
        if seconds <= 60:
            ticks = await self.hot_cache.get_recent(symbol, exchange, data_type, seconds)
            if ticks:
                latency_sec = time.perf_counter() - start_time
                latency_ms = latency_sec * 1000
                self.stats.hot_hits += 1
                
                # Record Prometheus metrics
                self.cache_hits.labels(
                    tier='HOT',
                    symbol=symbol,
                    exchange=exchange,
                    data_type=data_type
                ).inc()
                self.cache_latency.labels(
                    tier='HOT',
                    symbol=symbol,
                    exchange=exchange,
                    data_type=data_type
                ).observe(latency_sec)
                self.cache_size.labels(
                    tier='HOT',
                    symbol=symbol,
                    exchange=exchange,
                    data_type=data_type
                ).set(len(ticks))
                
                return ticks, 'HOT', latency_ms
            else:
                # HOT cache miss - record penalty (Opus strategic metric)
                # Calculate data staleness: how old is newest data vs what was requested
                fallback_start = time.perf_counter()
        
        # Try warm cache (1-5 minutes)
        if seconds <= 300:
            ticks = await self.warm_cache.get_recent(symbol, exchange, data_type, seconds)
            if ticks:
                latency_sec = time.perf_counter() - start_time
                latency_ms = latency_sec * 1000
                self.stats.warm_hits += 1
                
                # Track HOT miss penalty if we fell through from HOT
                if seconds <= 60:
                    fallback_latency = time.perf_counter() - fallback_start
                    self.hot_miss_penalty.labels(
                        symbol=symbol,
                        exchange=exchange,
                        data_type=data_type,
                        penalty_type='latency'
                    ).observe(fallback_latency)
                
                # Track WARM latency impact (Opus strategic metric)
                # Flag queries exceeding "Smart Speed" thresholds
                if latency_ms > 10:  # 10ms threshold
                    self.warm_latency_impact.labels(
                        symbol=symbol,
                        exchange=exchange,
                        data_type=data_type,
                        threshold_ms='10'
                    ).inc()
                if latency_ms > 20:  # 20ms threshold
                    self.warm_latency_impact.labels(
                        symbol=symbol,
                        exchange=exchange,
                        data_type=data_type,
                        threshold_ms='20'
                    ).inc()
                
                # Record Prometheus metrics
                self.cache_hits.labels(
                    tier='WARM',
                    symbol=symbol,
                    exchange=exchange,
                    data_type=data_type
                ).inc()
                self.cache_latency.labels(
                    tier='WARM',
                    symbol=symbol,
                    exchange=exchange,
                    data_type=data_type
                ).observe(latency_sec)
                self.cache_size.labels(
                    tier='WARM',
                    symbol=symbol,
                    exchange=exchange,
                    data_type=data_type
                ).set(len(ticks))
                
                return ticks, 'WARM', latency_ms
        
        # Fallback to cold storage (QuestDB)
        ticks = await self._query_questdb(symbol, exchange, data_type, seconds)
        latency_sec = time.perf_counter() - start_time
        latency_ms = latency_sec * 1000
        
        if ticks:
            self.stats.cold_hits += 1
            
            # Record Prometheus metrics
            self.cache_hits.labels(
                tier='COLD',
                symbol=symbol,
                exchange=exchange,
                data_type=data_type
            ).inc()
            self.cache_latency.labels(
                tier='COLD',
                symbol=symbol,
                exchange=exchange,
                data_type=data_type
            ).observe(latency_sec)
            self.cache_size.labels(
                tier='COLD',
                symbol=symbol,
                exchange=exchange,
                data_type=data_type
            ).set(len(ticks))
            
            return ticks, 'COLD', latency_ms
        else:
            self.stats.misses += 1
            
            # Record cache miss
            self.cache_latency.labels(
                tier='MISS',
                symbol=symbol,
                exchange=exchange,
                data_type=data_type
            ).observe(latency_sec)
            
            return [], 'MISS', latency_ms
    
    async def _query_questdb(
        self,
        symbol: str,
        exchange: str,
        data_type: str,
        seconds: int
    ) -> List[MarketTick]:
        """Query cold storage (QuestDB)"""
        if not self.db_pool:
            return []
        
        # Determine table name
        table_map = {
            'TRADE': 'market_trades',
            'ORDERBOOK': 'market_orderbook',
            'DEPTH': 'market_depth',
            'FUNDING': 'market_funding'
        }
        
        table_name = table_map.get(data_type)
        if not table_name:
            return []
        
        # Build query based on data type
        # Note: Using dateadd() instead of INTERVAL for QuestDB compatibility with prepared statements
        # Limit results to 10K to keep queries fast
        if data_type == 'TRADE':
            query = f"""
                SELECT timestamp, symbol, exchange, price, volume, side
                FROM {table_name}
                WHERE symbol = $1
                AND exchange = $2
                AND timestamp > dateadd('s', -{seconds}, now())
                ORDER BY timestamp DESC
                LIMIT 10000
            """
        elif data_type == 'ORDERBOOK':
            query = f"""
                SELECT timestamp, symbol, exchange, bid_price, bid_qty, ask_price, ask_qty
                FROM {table_name}
                WHERE symbol = $1
                AND exchange = $2
                AND timestamp > dateadd('s', -{seconds}, now())
                ORDER BY timestamp DESC
                LIMIT 10000
            """
        else:
            return []
        
        try:
            async with self.db_pool.acquire() as conn:
                rows = await conn.fetch(query, symbol, exchange)
                
                # Convert to MarketTick objects
                ticks = []
                for row in rows:
                    tick_data = dict(row)
                    tick_data['data_type'] = data_type
                    
                    # Handle trade data
                    if data_type == 'TRADE':
                        # Convert boolean side to string: True='BUY', False='SELL'
                        side_bool = tick_data.get('side')
                        side_str = 'BUY' if side_bool else 'SELL' if side_bool is not None else None
                        
                        tick = MarketTick(
                            timestamp=tick_data['timestamp'],
                            symbol=tick_data['symbol'],
                            exchange=tick_data['exchange'],
                            data_type=data_type,
                            price=tick_data.get('price'),
                            volume=tick_data.get('volume'),
                            side=side_str
                        )
                    # Handle orderbook data
                    elif data_type == 'ORDERBOOK':
                        tick = MarketTick(
                            timestamp=tick_data['timestamp'],
                            symbol=tick_data['symbol'],
                            exchange=tick_data['exchange'],
                            data_type=data_type,
                            bid_price=tick_data.get('bid_price'),
                            bid_qty=tick_data.get('bid_qty'),
                            ask_price=tick_data.get('ask_price'),
                            ask_qty=tick_data.get('ask_qty')
                        )
                    else:
                        continue
                    
                    ticks.append(tick)
                
                return ticks
        
        except Exception as e:
            print(f"âŒ QuestDB query error: {e}")
            return []
    
    def get_stats(self) -> Dict[str, Any]:
        """Get comprehensive cache statistics"""
        return {
            'cache_stats': self.stats.to_dict(),
            'hot_cache': self.hot_cache.get_stats(),
            'timestamp': datetime.utcnow().isoformat()
        }
    
    def reset_stats(self):
        """Reset statistics counters"""
        self.stats = CacheStats()


async def example_usage():
    """Example usage of TieredMarketData"""
    
    # Initialize
    tiered_data = TieredMarketData(
        questdb_host='localhost',
        questdb_port=8812,
        redis_host='localhost',
        redis_port=6379
    )
    
    await tiered_data.start()
    
    try:
        # Query recent trades (should hit hot cache if recent)
        ticks, tier, latency = await tiered_data.get_recent_ticks(
            symbol='BTCUSDT',
            exchange='binance_spot',
            data_type='TRADE',
            seconds=60
        )
        
        print(f"âœ… Got {len(ticks)} ticks from {tier} tier in {latency:.2f}ms")
        
        # Query older data (will hit warm or cold)
        ticks, tier, latency = await tiered_data.get_recent_ticks(
            symbol='BTCUSDT',
            exchange='binance_spot',
            data_type='TRADE',
            seconds=180
        )
        
        print(f"âœ… Got {len(ticks)} ticks from {tier} tier in {latency:.2f}ms")
        
        # Print statistics
        stats = tiered_data.get_stats()
        print(f"\nðŸ“Š Cache Statistics:")
        print(f"   Total Queries: {stats['cache_stats']['total_queries']}")
        print(f"   Cache Hit Rate: {stats['cache_stats']['cache_hit_rate_pct']}%")
        print(f"   Hot Hit Rate: {stats['cache_stats']['hot_hit_rate_pct']}%")
    
    finally:
        await tiered_data.stop()


if __name__ == '__main__':
    asyncio.run(example_usage())
