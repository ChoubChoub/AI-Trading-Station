#!/usr/bin/env python3
"""
QuestDB Guardian - Production Monitoring & Recovery System
===========================================================

Purpose: Detect and recover from QuestDB WAL corruption automatically
Author: AI Trading Station Team
Date: 2025-11-10
Version: 1.0

Features:
- Real-time WAL corruption detection (1-second cycles)
- Automated recovery using proven emergency script
- Prometheus metrics export
- Daily automated backups
- Health monitoring

Requirements:
- Python 3.10+
- prometheus-client
- Running QuestDB instance
- Existing emergency recovery script
"""

import os
import sys
import time
import json
import hashlib
import subprocess
import signal
import logging
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field

try:
    from prometheus_client import Counter, Histogram, Gauge, start_http_server, Info
except ImportError:
    print("ERROR: prometheus-client not installed")
    print("Run: pip install prometheus-client")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/home/youssefbahloul/ai-trading-station/Services/QuestDB/logs/guardian.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('QuestDBGuardian')

# Telegram Alert Configuration
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')

def send_telegram_alert(message: str, is_urgent: bool = False) -> bool:
    """
    Send alert to Telegram
    
    Returns:
        True if alert sent successfully, False otherwise
    """
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        logger.warning("Telegram not configured - skipping alert")
        return False
    
    try:
        # Add urgency indicator
        prefix = "üö® URGENT: " if is_urgent else "‚ÑπÔ∏è "
        full_message = f"{prefix}QuestDB Guardian: {message}"
        
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
        data = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": full_message,
            "parse_mode": "HTML"
        }
        
        response = requests.post(url, data=data, timeout=10)
        if response.status_code == 200:
            logger.info("Telegram alert sent successfully")
            return True
        else:
            logger.warning(f"Telegram alert failed: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"Failed to send Telegram alert: {e}")
        return False

def test_telegram_connectivity() -> bool:
    """
    Test Telegram connectivity at startup
    
    Returns:
        True if Telegram is working, False otherwise
    """
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        logger.warning("‚ö†Ô∏è  Telegram not configured - alerts will be disabled")
        logger.warning("   Set TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID environment variables")
        return False
    
    logger.info("Testing Telegram connectivity...")
    
    try:
        # Test bot token validity by calling getMe
        url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/getMe"
        response = requests.get(url, timeout=10)
        
        if response.status_code != 200:
            logger.error(f"‚ùå Telegram bot token invalid (status: {response.status_code})")
            return False
        
        bot_info = response.json()
        if not bot_info.get('ok'):
            logger.error(f"‚ùå Telegram API error: {bot_info.get('description', 'Unknown error')}")
            return False
        
        bot_name = bot_info.get('result', {}).get('username', 'Unknown')
        logger.info(f"‚úì Telegram bot verified: @{bot_name}")
        
        # Send test message
        test_message = "Guardian startup self-test - Telegram connectivity verified ‚úì"
        success = send_telegram_alert(test_message, is_urgent=False)
        
        if success:
            logger.info("‚úì Telegram self-test PASSED - alerts enabled")
            return True
        else:
            logger.error("‚ùå Telegram self-test FAILED - check chat ID and permissions")
            return False
            
    except requests.exceptions.Timeout:
        logger.error("‚ùå Telegram connection timeout - check network connectivity")
        return False
    except requests.exceptions.ConnectionError:
        logger.error("‚ùå Telegram connection error - check network/firewall")
        return False
    except Exception as e:
        logger.error(f"‚ùå Telegram self-test failed: {e}")
        return False

# Prometheus Metrics
corruption_detected = Counter(
    'questdb_corruption_events_total',
    'Total number of corruption events detected',
    ['table', 'corruption_type']
)

recovery_time = Histogram(
    'questdb_recovery_duration_seconds',
    'Time taken to recover from corruption',
    ['table', 'recovery_method']
)

wal_health = Gauge(
    'questdb_wal_health',
    'WAL health status (1=healthy, 0=corrupted)',
    ['table']
)

backup_age = Gauge(
    'questdb_last_backup_hours',
    'Hours since last successful backup',
    ['table']
)

scan_duration = Histogram(
    'questdb_scan_duration_seconds',
    'Time taken for corruption scan',
    ['table']
)

guardian_info = Info(
    'questdb_guardian',
    'Guardian version and configuration'
)

total_scans = Counter(
    'questdb_scans_total',
    'Total number of corruption scans performed'
)

false_positives = Counter(
    'questdb_false_positives_total',
    'False positive detections'
)

@dataclass
class CorruptionEvent:
    """Record of a corruption event"""
    timestamp: datetime
    table: str
    corruption_type: str
    details: str
    recovered: bool
    recovery_time: float
    backup_path: Optional[str] = None

class QuestDBGuardian:
    """
    Production Guardian for QuestDB corruption detection and recovery
    """
    
    def __init__(self):
        self.config = {
            'questdb_data': '/home/youssefbahloul/ai-trading-station/Services/QuestDB/data/hot/db',
            'questdb_url': 'http://localhost:9000',
            'backup_dir': '/home/youssefbahloul/ai-trading-station/Services/QuestDB/backups',
            'emergency_script': '/home/youssefbahloul/ai-trading-station/Services/QuestDB/Runtime/fix_wal_emergency_corrected.sh',
            'scan_interval': 5.0,  # seconds (reduced from 1.0 to prevent aggressive detection)
            'tables': ['market_trades', 'market_orderbook'],
            'corruption_threshold': 0.1,  # 10% zeros = corrupted
            'max_recovery_attempts': 3,
            'services_to_stop': ['batch-writer.service'],
            'services_to_restart': ['batch-writer.service'],
        }
        
        # Override with environment variables if provided
        if 'GUARDIAN_SCAN_INTERVAL' in os.environ:
            try:
                self.config['scan_interval'] = float(os.environ['GUARDIAN_SCAN_INTERVAL'])
                logger.info(f"Using GUARDIAN_SCAN_INTERVAL={self.config['scan_interval']}s from environment")
            except ValueError:
                logger.warning(f"Invalid GUARDIAN_SCAN_INTERVAL value: {os.environ['GUARDIAN_SCAN_INTERVAL']}, using default")
        
        if 'GUARDIAN_MAX_RECOVERY_ATTEMPTS' in os.environ:
            try:
                self.config['max_recovery_attempts'] = int(os.environ['GUARDIAN_MAX_RECOVERY_ATTEMPTS'])
                logger.info(f"Using GUARDIAN_MAX_RECOVERY_ATTEMPTS={self.config['max_recovery_attempts']} from environment")
            except ValueError:
                logger.warning(f"Invalid GUARDIAN_MAX_RECOVERY_ATTEMPTS value: {os.environ['GUARDIAN_MAX_RECOVERY_ATTEMPTS']}, using default")
        
        # Runtime state
        self.corruption_events: List[CorruptionEvent] = []
        self.last_scan_time = {}
        self.last_recovery_time = {}  # Track when tables were last recovered
        self.recovery_attempts = {}  # Track recovery attempts per table: {table_name: [(timestamp, success), ...]}
        self.circuit_breaker_timeout = 3600  # 1 hour window for circuit breaker
        
        # Recovery cooldown - override from environment if provided
        self.recovery_cooldown = 30  # Default: 30 seconds
        if 'GUARDIAN_COOLDOWN_SECONDS' in os.environ:
            try:
                self.recovery_cooldown = int(os.environ['GUARDIAN_COOLDOWN_SECONDS'])
                logger.info(f"Using GUARDIAN_COOLDOWN_SECONDS={self.recovery_cooldown}s from environment")
            except ValueError:
                logger.warning(f"Invalid GUARDIAN_COOLDOWN_SECONDS value: {os.environ['GUARDIAN_COOLDOWN_SECONDS']}, using default")
        
        # New table grace period - override from environment if provided
        self.new_table_grace_seconds = 10  # Default: 10 seconds
        if 'GUARDIAN_NEW_TABLE_GRACE_SECONDS' in os.environ:
            try:
                self.new_table_grace_seconds = int(os.environ['GUARDIAN_NEW_TABLE_GRACE_SECONDS'])
                logger.info(f"Using GUARDIAN_NEW_TABLE_GRACE_SECONDS={self.new_table_grace_seconds}s from environment")
            except ValueError:
                logger.warning(f"Invalid GUARDIAN_NEW_TABLE_GRACE_SECONDS value: {os.environ['GUARDIAN_NEW_TABLE_GRACE_SECONDS']}, using default")
        
        self.running = True
        self.recovery_in_progress = False
        
        # Validate configuration
        self.validate_setup()
        
        # Set Guardian info
        guardian_info.info({
            'version': '1.0',
            'scan_interval': str(self.config['scan_interval']),
            'tables_monitored': ','.join(self.config['tables']),
            'emergency_script': self.config['emergency_script']
        })
        
        logger.info("QuestDB Guardian initialized")
    
    def validate_setup(self):
        """Validate that all required components exist"""
        
        # Check QuestDB data directory
        if not os.path.exists(self.config['questdb_data']):
            raise RuntimeError(f"QuestDB data directory not found: {self.config['questdb_data']}")
        
        # Check emergency script
        if not os.path.exists(self.config['emergency_script']):
            raise RuntimeError(f"Emergency script not found: {self.config['emergency_script']}")
        
        # Create backup directories
        Path(f"{self.config['backup_dir']}/emergency").mkdir(parents=True, exist_ok=True)
        Path(f"{self.config['backup_dir']}/daily").mkdir(parents=True, exist_ok=True)
        Path(f"{self.config['backup_dir']}/metadata").mkdir(parents=True, exist_ok=True)
        
        # Create log directory
        Path('/home/youssefbahloul/ai-trading-station/Services/QuestDB/logs').mkdir(parents=True, exist_ok=True)
        
        # Test Telegram connectivity
        logger.info("=" * 60)
        telegram_ok = test_telegram_connectivity()
        if telegram_ok:
            logger.info("‚úì Telegram alerts enabled and tested")
        else:
            logger.warning("‚ö†Ô∏è  Telegram alerts disabled - Guardian will run without notifications")
        logger.info("=" * 60)
        
        logger.info("Configuration validated successfully")
    
    # ==================== CORRUPTION DETECTION ====================
    
    def find_table_directory(self, table_name: str) -> Optional[str]:
        """Find the actual table directory - returns the highest version number"""
        try:
            matching_dirs = []
            for item in os.listdir(self.config['questdb_data']):
                if item.startswith(f"{table_name}~"):
                    try:
                        # Extract version number from name like "market_trades~116"
                        version = int(item.split('~')[1])
                        full_path = os.path.join(self.config['questdb_data'], item)
                        if os.path.isdir(full_path):
                            matching_dirs.append((version, full_path, item))
                    except (ValueError, IndexError):
                        logger.warning(f"Skipping invalid table directory name: {item}")
                        continue
            
            if matching_dirs:
                # Sort by version number (highest first)
                matching_dirs.sort(key=lambda x: x[0], reverse=True)
                logger.debug(f"Found {len(matching_dirs)} versions of {table_name}, using {matching_dirs[0][2]}")
                return matching_dirs[0][1]
            
            logger.warning(f"No table directory found for {table_name}")
            return None
                
        except Exception as e:
            logger.error(f"Error finding table directory for {table_name}: {e}")
            return None
    
    def detect_wal_corruption(self, table_name: str) -> Tuple[bool, str]:
        """
        Detect WAL corruption using pattern analysis
        
        Returns:
            (is_corrupted, reason)
        """
        scan_start = time.time()
        
        # Check if table is in recovery cooldown period
        if table_name in self.last_recovery_time:
            time_since_recovery = time.time() - self.last_recovery_time[table_name]
            if time_since_recovery < self.recovery_cooldown:
                # Skip checking - table is in cooldown period
                scan_duration.labels(table=table_name).observe(time.time() - scan_start)
                logger.debug(f"Skipping {table_name} - in cooldown period ({time_since_recovery:.1f}s / {self.recovery_cooldown}s)")
                return False, f"In recovery cooldown ({self.recovery_cooldown - time_since_recovery:.0f}s remaining)"
        
        try:
            table_dir = self.find_table_directory(table_name)
            if not table_dir:
                scan_duration.labels(table=table_name).observe(time.time() - scan_start)
                return True, "Table directory not found"
            
            txn_seq_path = os.path.join(table_dir, 'txn_seq')
            if not os.path.exists(txn_seq_path):
                # Check table directory age - if it's very new, it's probably just being created
                table_dir_age = time.time() - os.path.getmtime(table_dir)
                if table_dir_age < self.new_table_grace_seconds:
                    logger.debug(f"{table_name} is new (age: {table_dir_age:.1f}s), txn_seq not yet created - this is normal")
                    scan_duration.labels(table=table_name).observe(time.time() - scan_start)
                    return False, "New table - txn_seq not yet created"
                scan_duration.labels(table=table_name).observe(time.time() - scan_start)
                return True, "txn_seq directory missing"
            
            txnlog_path = os.path.join(txn_seq_path, '_txnlog')
            if not os.path.exists(txnlog_path):
                scan_duration.labels(table=table_name).observe(time.time() - scan_start)
                return True, "_txnlog file missing"
            
            # Check file size
            file_size = os.path.getsize(txnlog_path)
            if file_size == 0:
                scan_duration.labels(table=table_name).observe(time.time() - scan_start)
                return True, "_txnlog is empty"
            
            # Check for zero-fill corruption (fast check on tail of file)
            with open(txnlog_path, 'rb') as f:
                # Read last 64KB for fast detection
                sample_size = min(65536, file_size)
                f.seek(max(0, file_size - sample_size))
                tail_data = f.read()
                
                if not tail_data:
                    scan_duration.labels(table=table_name).observe(time.time() - scan_start)
                    return True, "Unable to read file data"
                
                # Count zero bytes
                zero_count = tail_data.count(b'\x00')
                zero_ratio = zero_count / len(tail_data)
                
                if zero_ratio > self.config['corruption_threshold']:
                    scan_duration.labels(table=table_name).observe(time.time() - scan_start)
                    return True, f"Zero-fill corruption detected ({zero_ratio*100:.1f}% zeros)"
            
            scan_duration.labels(table=table_name).observe(time.time() - scan_start)
            return False, "Healthy"
            
        except Exception as e:
            scan_duration.labels(table=table_name).observe(time.time() - scan_start)
            logger.error(f"Error detecting corruption in {table_name}: {e}")
            return True, f"Detection error: {str(e)}"
    
    def scan_all_tables(self) -> Dict[str, Tuple[bool, str]]:
        """Scan all configured tables for corruption"""
        
        total_scans.inc()
        results = {}
        
        for table_name in self.config['tables']:
            corrupted, reason = self.detect_wal_corruption(table_name)
            results[table_name] = (corrupted, reason)
            
            # Update Prometheus metrics
            if corrupted:
                wal_health.labels(table=table_name).set(0)
                logger.warning(f"Corruption detected in {table_name}: {reason}")
            else:
                wal_health.labels(table=table_name).set(1)
        
        return results
    
    # ==================== RECOVERY SYSTEM ====================
    
    def check_service_status(self, service_name: str) -> str:
        """
        Check systemd service status
        Returns: 'active', 'inactive', 'failed', 'unknown'
        """
        try:
            result = subprocess.run(
                ['systemctl', 'is-active', service_name],
                capture_output=True,
                text=True,
                timeout=5
            )
            status = result.stdout.strip()
            return status if status in ['active', 'inactive', 'failed'] else 'unknown'
        except subprocess.TimeoutExpired:
            logger.warning(f"Timeout checking status of {service_name}")
            return 'unknown'
        except Exception as e:
            logger.error(f"Error checking service status for {service_name}: {e}")
            return 'unknown'
    
    def stop_services(self):
        """Stop data ingestion services before recovery"""
        logger.info("Stopping services for recovery...")
        
        for service in self.config['services_to_stop']:
            # Check service status first
            status = self.check_service_status(service)
            
            if status == 'inactive':
                logger.info(f"‚äò {service} already stopped, skipping")
                continue
            elif status == 'unknown':
                logger.warning(f"‚ö† Cannot determine status of {service}, attempting stop anyway")
            
            try:
                result = subprocess.run(
                    ['sudo', 'systemctl', 'stop', service],
                    capture_output=True,
                    timeout=15,
                    text=True
                )
                
                if result.returncode == 0:
                    logger.info(f"‚úì Stopped {service}")
                else:
                    logger.warning(f"Failed to stop {service}: {result.stderr}")
                    
            except subprocess.TimeoutExpired:
                logger.error(f"Timeout stopping {service}")
            except Exception as e:
                logger.error(f"Error stopping {service}: {e}")
    
    def restart_services(self):
        """Restart data ingestion services after recovery"""
        logger.info("Restarting services...")
        
        time.sleep(2)  # Brief pause before restart
        
        for service in self.config['services_to_restart']:
            try:
                result = subprocess.run(
                    ['sudo', 'systemctl', 'start', service],
                    capture_output=True,
                    timeout=20,
                    text=True
                )
                
                if result.returncode == 0:
                    logger.info(f"‚úì Started {service}")
                else:
                    logger.warning(f"Failed to start {service}: {result.stderr}")
                    
            except subprocess.TimeoutExpired:
                logger.error(f"Timeout starting {service}")
            except Exception as e:
                logger.error(f"Error starting {service}: {e}")
    
    def verify_table_health(self, table_name: str, max_wait: int = 30) -> Tuple[bool, str]:
        """
        Verify table health after recovery
        
        Checks:
        1. Table directory exists
        2. txn_seq directory exists
        3. WAL corruption check passes
        4. Table responds to health query
        
        Args:
            table_name: Name of table to verify
            max_wait: Maximum seconds to wait for table to become healthy
        
        Returns:
            (is_healthy, reason)
        """
        start_time = time.time()
        
        logger.info(f"Verifying health of {table_name}...")
        
        # Give table time to initialize after recovery
        time.sleep(2)
        
        while (time.time() - start_time) < max_wait:
            # Check 1: Table directory exists
            table_dir = self.find_table_directory(table_name)
            if not table_dir:
                logger.warning(f"{table_name}: Table directory not found, waiting...")
                time.sleep(2)
                continue
            
            # Check 2: txn_seq directory exists
            txn_seq_path = os.path.join(table_dir, 'txn_seq')
            if not os.path.exists(txn_seq_path):
                elapsed = time.time() - start_time
                if elapsed < 10:  # Grace period for txn_seq creation
                    logger.debug(f"{table_name}: txn_seq not yet created (elapsed: {elapsed:.1f}s)")
                    time.sleep(2)
                    continue
                else:
                    return False, f"txn_seq missing after {elapsed:.1f}s"
            
            # Check 3: WAL corruption check
            corrupted, reason = self.detect_wal_corruption(table_name)
            if corrupted:
                return False, f"Still corrupted: {reason}"
            
            # Check 4: Query table to verify responsiveness
            try:
                response = requests.get(
                    f"{self.config['questdb_url']}/exec",
                    params={'query': f"SELECT count() FROM {table_name}"},
                    timeout=5
                )
                
                if response.status_code == 200:
                    logger.info(f"‚úì {table_name} is healthy and responsive")
                    return True, "Table healthy"
                else:
                    logger.warning(f"{table_name}: Query returned status {response.status_code}")
                    time.sleep(2)
                    continue
                    
            except requests.exceptions.RequestException as e:
                logger.warning(f"{table_name}: Query failed: {e}, retrying...")
                time.sleep(2)
                continue
        
        # Timeout exceeded
        elapsed = time.time() - start_time
        return False, f"Health verification timeout after {elapsed:.1f}s"
    
    def check_circuit_breaker(self, table_name: str) -> Tuple[bool, str]:
        """
        Circuit breaker to prevent repeated recovery attempts
        
        Checks if table has exceeded max recovery attempts within the time window.
        Automatically cleans up old attempts outside the window.
        
        Returns:
            (can_recover, reason)
        """
        current_time = time.time()
        
        # Initialize tracking for this table if needed
        if table_name not in self.recovery_attempts:
            self.recovery_attempts[table_name] = []
        
        # Clean up old attempts outside the time window
        cutoff_time = current_time - self.circuit_breaker_timeout
        self.recovery_attempts[table_name] = [
            (timestamp, success) for timestamp, success in self.recovery_attempts[table_name]
            if timestamp > cutoff_time
        ]
        
        # Count recent attempts
        recent_attempts = len(self.recovery_attempts[table_name])
        max_attempts = self.config['max_recovery_attempts']
        
        if recent_attempts >= max_attempts:
            # Check if any recent attempts succeeded
            recent_successes = sum(1 for _, success in self.recovery_attempts[table_name] if success)
            
            if recent_successes == 0:
                # All attempts failed - circuit breaker OPEN
                oldest_attempt = min(t for t, _ in self.recovery_attempts[table_name])
                time_until_reset = int((oldest_attempt + self.circuit_breaker_timeout) - current_time)
                
                logger.error(
                    f"üö® Circuit breaker OPEN for {table_name}: "
                    f"{recent_attempts}/{max_attempts} attempts failed in last hour. "
                    f"Resets in {time_until_reset}s"
                )
                
                return False, f"Circuit breaker: {recent_attempts}/{max_attempts} failed attempts"
            else:
                # Some attempts succeeded - allow retry but warn
                logger.warning(
                    f"‚ö†Ô∏è {table_name} has {recent_attempts}/{max_attempts} attempts "
                    f"({recent_successes} succeeded) in last hour"
                )
                return True, "Circuit breaker: allowing retry (some successes)"
        
        # Under threshold - circuit breaker CLOSED (normal operation)
        return True, f"Circuit breaker: {recent_attempts}/{max_attempts} attempts"
    
    def record_recovery_attempt(self, table_name: str, success: bool):
        """Record a recovery attempt for circuit breaker tracking"""
        current_time = time.time()
        
        if table_name not in self.recovery_attempts:
            self.recovery_attempts[table_name] = []
        
        self.recovery_attempts[table_name].append((current_time, success))
        
        status = "SUCCESS" if success else "FAILED"
        logger.info(f"üìä Recorded recovery attempt for {table_name}: {status}")
    
    def create_emergency_backup(self, table_name: str) -> Optional[str]:
        """Create emergency backup before recovery attempt"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        table_dir = self.find_table_directory(table_name)
        
        if not table_dir:
            logger.error(f"Cannot backup {table_name}: directory not found")
            return None
        
        backup_path = f"{self.config['backup_dir']}/emergency/{table_name}_{timestamp}.tar.zst"
        
        try:
            logger.info(f"Creating emergency backup: {backup_path}")
            
            result = subprocess.run([
                'tar', '--zstd', '-cf', backup_path,
                '-C', os.path.dirname(table_dir),
                os.path.basename(table_dir)
            ], capture_output=True, timeout=120, text=True)
            
            if result.returncode == 0:
                size_mb = os.path.getsize(backup_path) / (1024 * 1024)
                logger.info(f"‚úì Backup created: {backup_path} ({size_mb:.1f} MB)")
                return backup_path
            else:
                logger.error(f"Backup failed: {result.stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            logger.error("Backup timed out (>120s)")
            return None
        except Exception as e:
            logger.error(f"Backup error: {e}")
            return None
    
    def execute_emergency_recovery(self) -> bool:
        """Execute the proven emergency recovery script"""
        
        logger.info("Executing emergency recovery script...")
        
        try:
            result = subprocess.run(
                ['bash', self.config['emergency_script']],
                capture_output=True,
                timeout=90,
                text=True
            )
            
            if result.returncode == 0:
                logger.info("‚úì Emergency script completed successfully")
                logger.info(f"Output: {result.stdout}")
                return True
            else:
                logger.error(f"‚úó Emergency script failed: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("Emergency script timed out (>90s)")
            return False
        except Exception as e:
            logger.error(f"Emergency script error: {e}")
            return False
    
    def recover_from_corruption(self, table_name: str, corruption_type: str) -> bool:
        """
        Complete recovery workflow
        
        Returns:
            True if recovery successful, False otherwise
        """
        
        if self.recovery_in_progress:
            logger.warning("Recovery already in progress, skipping...")
            return False
        
        # Check circuit breaker before attempting recovery
        can_recover, breaker_reason = self.check_circuit_breaker(table_name)
        if not can_recover:
            logger.error(f"üö´ Circuit breaker preventing recovery: {breaker_reason}")
            send_telegram_alert(
                f"Circuit breaker OPEN for '{table_name}': {breaker_reason}. Manual intervention required!",
                is_urgent=True
            )
            return False
        
        logger.info(f"‚úì Circuit breaker check passed: {breaker_reason}")
        
        self.recovery_in_progress = True
        recovery_start = time.time()
        
        try:
            logger.critical(f"‚ö†Ô∏è  STARTING RECOVERY for {table_name}: {corruption_type}")
            
            # 1. Stop services
            self.stop_services()
            time.sleep(1)
            
            # 2. Create backup
            backup_path = self.create_emergency_backup(table_name)
            
            # 3. Execute recovery script
            recovery_success = self.execute_emergency_recovery()
            
            # 4. Restart services
            self.restart_services()
            
            # 5. Verify table health after recovery
            if recovery_success:
                logger.info(f"Verifying {table_name} health after recovery...")
                is_healthy, health_reason = self.verify_table_health(table_name, max_wait=30)
                
                if not is_healthy:
                    logger.error(f"‚úó Health verification failed: {health_reason}")
                    recovery_success = False
                else:
                    logger.info(f"‚úì Health verification passed: {health_reason}")
            
            # Record metrics
            recovery_duration = time.time() - recovery_start
            recovery_time.labels(
                table=table_name,
                recovery_method='emergency_script'
            ).observe(recovery_duration)
            
            # Log event
            event = CorruptionEvent(
                timestamp=datetime.now(),
                table=table_name,
                corruption_type=corruption_type,
                details=f"Recovery duration: {recovery_duration:.1f}s",
                recovered=recovery_success,
                recovery_time=recovery_duration,
                backup_path=backup_path
            )
            self.corruption_events.append(event)
            
            # Log to file
            self.log_corruption_event(event)
            
            # Record recovery attempt for circuit breaker
            self.record_recovery_attempt(table_name, recovery_success)
            
            if recovery_success:
                logger.info(f"‚úì Recovery completed in {recovery_duration:.1f}s")
                # Record recovery timestamp to enable cooldown period
                self.last_recovery_time[table_name] = time.time()
                logger.info(f"‚úì Cooldown period of {self.recovery_cooldown}s started for {table_name}")
                send_telegram_alert(f"Recovery successful for table '{table_name}' in {recovery_duration:.1f}s", is_urgent=False)
            else:
                logger.error(f"‚úó Recovery failed after {recovery_duration:.1f}s")
                send_telegram_alert(f"Recovery FAILED for table '{table_name}' after {recovery_duration:.1f}s - Manual intervention required!", is_urgent=True)
            
            return recovery_success
            
        except Exception as e:
            logger.critical(f"Recovery exception: {e}")
            # Record failed attempt
            self.record_recovery_attempt(table_name, False)
            return False
        finally:
            self.recovery_in_progress = False
    
    def log_corruption_event(self, event: CorruptionEvent):
        """Log corruption event to file for audit trail with enhanced context"""
        
        log_file = f"{self.config['backup_dir']}/corruption_events.jsonl"
        
        try:
            # Get circuit breaker status for context
            table_name = event.table
            recent_attempts = []
            if table_name in self.recovery_attempts:
                cutoff_time = time.time() - self.circuit_breaker_timeout
                recent_attempts = [
                    {
                        'timestamp': datetime.fromtimestamp(ts).isoformat(),
                        'success': success
                    }
                    for ts, success in self.recovery_attempts[table_name]
                    if ts > cutoff_time
                ]
            
            with open(log_file, 'a') as f:
                event_data = {
                    'timestamp': event.timestamp.isoformat(),
                    'table': event.table,
                    'corruption_type': event.corruption_type,
                    'details': event.details,
                    'recovered': event.recovered,
                    'recovery_time': event.recovery_time,
                    'backup_path': event.backup_path,
                    # Enhanced context
                    'circuit_breaker': {
                        'recent_attempts_count': len(recent_attempts),
                        'max_attempts': self.config['max_recovery_attempts'],
                        'window_seconds': self.circuit_breaker_timeout,
                        'recent_attempts': recent_attempts
                    },
                    'guardian_config': {
                        'scan_interval': self.config['scan_interval'],
                        'recovery_cooldown': self.recovery_cooldown,
                        'new_table_grace': self.new_table_grace_seconds,
                        'corruption_threshold': self.config['corruption_threshold']
                    }
                }
                f.write(json.dumps(event_data) + '\n')
                
            # Also log summary to console for visibility
            logger.info(
                f"üìù Event logged: {event.table} | "
                f"Recovered: {event.recovered} | "
                f"Duration: {event.recovery_time:.1f}s | "
                f"Recent attempts: {len(recent_attempts)}/{self.config['max_recovery_attempts']}"
            )
            
        except Exception as e:
            logger.error(f"Failed to log event: {e}")
    
    # ==================== BACKUP SYSTEM ====================
    
    def update_backup_metrics(self):
        """Update backup age metrics from filesystem"""
        
        daily_backup_dir = f"{self.config['backup_dir']}/daily"
        
        for table_name in self.config['tables']:
            try:
                # Find latest backup for this table
                backups = list(Path(daily_backup_dir).glob(f"{table_name}_*.tar.zst"))
                
                if backups:
                    latest_backup = max(backups, key=lambda p: p.stat().st_mtime)
                    age_seconds = time.time() - latest_backup.stat().st_mtime
                    age_hours = age_seconds / 3600
                    
                    backup_age.labels(table=table_name).set(age_hours)
                else:
                    # No backups found
                    backup_age.labels(table=table_name).set(999)
                    
            except Exception as e:
                logger.error(f"Error updating backup metrics for {table_name}: {e}")
    
    # ==================== MAIN MONITORING LOOP ====================
    
    def monitor_loop(self):
        """Main monitoring loop"""
        
        # Enhanced startup logging
        logger.info("=" * 80)
        logger.info("üõ°Ô∏è  QuestDB Guardian Starting - Phase 2 Enhanced")
        logger.info("=" * 80)
        logger.info(f"üìä Configuration:")
        logger.info(f"   - Scan interval: {self.config['scan_interval']}s")
        logger.info(f"   - Recovery cooldown: {self.recovery_cooldown}s")
        logger.info(f"   - New table grace: {self.new_table_grace_seconds}s")
        logger.info(f"   - Max recovery attempts: {self.config['max_recovery_attempts']}")
        logger.info(f"   - Circuit breaker window: {self.circuit_breaker_timeout}s ({self.circuit_breaker_timeout/3600:.1f}h)")
        logger.info(f"   - Corruption threshold: {self.config['corruption_threshold']*100}%")
        logger.info(f"üìã Monitoring tables: {', '.join(self.config['tables'])}")
        logger.info(f"üîß Phase 2 Features: Health Verification ‚úì | Circuit Breaker ‚úì | Enhanced Logging ‚úì")
        logger.info("=" * 80)
        
        consecutive_corruption_count = {}
        
        while self.running:
            try:
                # Scan all tables
                results = self.scan_all_tables()
                
                # Check for corruption
                for table_name, (corrupted, reason) in results.items():
                    if corrupted:
                        # Track consecutive corruption detections
                        consecutive_corruption_count[table_name] = \
                            consecutive_corruption_count.get(table_name, 0) + 1
                        
                        corruption_detected.labels(
                            table=table_name,
                            corruption_type=reason
                        ).inc()
                        
                        # Trigger recovery on first detection
                        if consecutive_corruption_count[table_name] == 1:
                            logger.critical(f"‚ö†Ô∏è  CORRUPTION DETECTED: {table_name} - {reason}")
                            send_telegram_alert(f"WAL corruption detected in table '{table_name}': {reason}", is_urgent=True)
                            
                            # Attempt recovery
                            recovery_success = self.recover_from_corruption(table_name, reason)
                            
                            if recovery_success:
                                # Reset counter on successful recovery
                                consecutive_corruption_count[table_name] = 0
                            else:
                                logger.error("Recovery failed - will retry on next scan")
                        
                        elif consecutive_corruption_count[table_name] >= 3:
                            logger.critical(
                                f"‚ö†Ô∏è  PERSISTENT CORRUPTION: {table_name} failed recovery "
                                f"{consecutive_corruption_count[table_name]} times"
                            )
                            # Reset to avoid log spam
                            consecutive_corruption_count[table_name] = 0
                    else:
                        # Reset counter on healthy scan
                        consecutive_corruption_count[table_name] = 0
                
                # Update backup age metrics
                self.update_backup_metrics()
                
                # Sleep until next scan
                time.sleep(self.config['scan_interval'])
                
            except KeyboardInterrupt:
                logger.info("Shutdown signal received")
                self.running = False
                break
            except Exception as e:
                logger.error(f"Monitor loop error: {e}")
                time.sleep(5)  # Brief pause on error
        
        logger.info("Guardian monitoring stopped")
    
    def shutdown(self, signum, frame):
        """Graceful shutdown handler"""
        logger.info(f"Received signal {signum}, shutting down gracefully...")
        self.running = False

def main():
    """Main entry point"""
    
    # Check if running as correct user
    if os.geteuid() == 0:
        logger.warning("Running as root - this is not recommended")
    
    try:
        # Initialize Guardian
        guardian = QuestDBGuardian()
        
        # Setup signal handlers
        signal.signal(signal.SIGTERM, guardian.shutdown)
        signal.signal(signal.SIGINT, guardian.shutdown)
        
        # Start Prometheus metrics server
        logger.info("Starting Prometheus metrics server on port 9093...")
        start_http_server(9093)
        
        # Start monitoring
        guardian.monitor_loop()
        
    except Exception as e:
        logger.critical(f"Fatal error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
